{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7541723-4f47-40a8-87d8-15492f2c0a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from scipy import stats\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.manifold import TSNE\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.stattools import adfuller, kpss\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "import geopandas as gpd\n",
    "from pyproj import CRS\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "print(f\"EDA started at: {datetime.now()}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3042ab-788e-4a67-b512-458bfd00b143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cleaned Data \n",
    "\n",
    "try:\n",
    "    df = pd.read_parquet('cleaned_agricultural_data.parquet')\n",
    "except:\n",
    "    df = pd.read_csv('cleaned_agricultural_data.csv')\n",
    "\n",
    "print(f\"Dataset loaded: {df.shape}\")\n",
    "print(f\"Years: {df['Year'].min()} to {df['Year'].max()}\")\n",
    "print(f\"States: {df['State Name'].nunique()}\")\n",
    "print(f\"Districts: {df['Dist Name'].nunique()}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c97290-3f08-4c93-ba6b-5e54d2ce1152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.Statistical and Distribution Analysis\n",
    "\n",
    "def comprehensive_statistical_summary(df):\n",
    "    \"\"\"Generate comprehensive statistical summary\"\"\"\n",
    "    \n",
    "    # Basic statistics\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    basic_stats = df[numeric_cols].describe().T\n",
    "    basic_stats['skewness'] = df[numeric_cols].skew()\n",
    "    basic_stats['kurtosis'] = df[numeric_cols].kurtosis()\n",
    "    basic_stats['cv'] = basic_stats['std'] / basic_stats['mean']  # Coefficient of variation\n",
    "    \n",
    "    # Normality tests for key columns\n",
    "    key_columns = [\n",
    "        'OVERALL_YIELD_Kg_per_ha',\n",
    "        'TOTAL_PRODUCTION_1000_tons',\n",
    "        'RICE YIELD (Kg per ha)',\n",
    "        'WHEAT YIELD (Kg per ha)'\n",
    "    ]\n",
    "    \n",
    "    normality_results = {}\n",
    "    \n",
    "    for col in key_columns:\n",
    "        if col in df.columns:\n",
    "            col_data = df[col].dropna()\n",
    "            \n",
    "            # Shapiro max is 5000 samples\n",
    "            sample_size = min(len(col_data), 5000)\n",
    "            \n",
    "            # Only sample if we have more than 3 values\n",
    "            if sample_size > 3:\n",
    "                sample = col_data.sample(sample_size, replace=False)\n",
    "                stat, p_value = stats.shapiro(sample)\n",
    "                normality_results[col] = {\n",
    "                    'statistic': stat,\n",
    "                    'p_value': p_value\n",
    "                }\n",
    "            else:\n",
    "                normality_results[col] = {\n",
    "                    'statistic': None,\n",
    "                    'p_value': None\n",
    "                }\n",
    "    \n",
    "    return basic_stats, normality_results\n",
    "\n",
    "\n",
    "# Generate statistics\n",
    "basic_stats, normality_results = comprehensive_statistical_summary(df)\n",
    "\n",
    "print(\"Statistical Summary (Key Columns):\")\n",
    "print(\"=\" * 80)\n",
    "print(basic_stats[['mean', 'std', 'min', '50%', 'max', 'skewness', 'cv']].head(10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2889ba2-4a0a-4413-ac06-39b0256b3e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Time series Analysis and Trend Detection\n",
    "\n",
    "def analyze_temporal_trends(df):\n",
    "    \"\"\"Analyze temporal trends for key metrics\"\"\"\n",
    "    \n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=('Overall Yield Trend', 'Total Production Trend',\n",
    "                       'Rice Yield Trend', 'Wheat Yield Trend'),\n",
    "        vertical_spacing=0.15,\n",
    "        horizontal_spacing=0.1\n",
    "    )\n",
    "    \n",
    "    # Overall yield trend\n",
    "    yearly_yield = df.groupby('Year')['OVERALL_YIELD_Kg_per_ha'].mean().reset_index()\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=yearly_yield['Year'], y=yearly_yield['OVERALL_YIELD_Kg_per_ha'],\n",
    "                  mode='lines+markers', name='Overall Yield'),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Total production trend\n",
    "    yearly_prod = df.groupby('Year')['TOTAL_PRODUCTION_1000_tons'].sum().reset_index()\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=yearly_prod['Year'], y=yearly_prod['TOTAL_PRODUCTION_1000_tons'],\n",
    "                  mode='lines+markers', name='Total Production', line=dict(color='red')),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # Rice yield trend\n",
    "    if 'RICE YIELD (Kg per ha)' in df.columns:\n",
    "        rice_yield = df.groupby('Year')['RICE YIELD (Kg per ha)'].mean().reset_index()\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=rice_yield['Year'], y=rice_yield['RICE YIELD (Kg per ha)'],\n",
    "                      mode='lines+markers', name='Rice Yield', line=dict(color='green')),\n",
    "            row=2, col=1\n",
    "        )\n",
    "    \n",
    "    # Wheat yield trend\n",
    "    if 'WHEAT YIELD (Kg per ha)' in df.columns:\n",
    "        wheat_yield = df.groupby('Year')['WHEAT YIELD (Kg per ha)'].mean().reset_index()\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=wheat_yield['Year'], y=wheat_yield['WHEAT YIELD (Kg per ha)'],\n",
    "                      mode='lines+markers', name='Wheat Yield', line=dict(color='orange')),\n",
    "            row=2, col=2\n",
    "        )\n",
    "    \n",
    "    fig.update_layout(height=800, showlegend=True, title_text=\"Temporal Trends Analysis\")\n",
    "    fig.show()\n",
    "    \n",
    "    # Perform structural break detection\n",
    "    print(\"\\nStructural Break Analysis (Chow Test):\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for col in ['OVERALL_YIELD_Kg_per_ha', 'TOTAL_PRODUCTION_1000_tons']:\n",
    "        if col in df.columns:\n",
    "            # Simple Chow test for 2013 (mid-point)\n",
    "            pre_2013 = df[df['Year'] < 2013][col].dropna()\n",
    "            post_2013 = df[df['Year'] >= 2013][col].dropna()\n",
    "            \n",
    "            if len(pre_2013) > 10 and len(post_2013) > 10:\n",
    "                f_stat, p_value = stats.f_oneway(pre_2013, post_2013)\n",
    "                print(f\"{col}: F-statistic = {f_stat:.4f}, p-value = {p_value:.4f}\")\n",
    "                if p_value < 0.05:\n",
    "                    print(f\"  → Significant structural break detected at 2013\")\n",
    "\n",
    "analyze_temporal_trends(df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8815fcfa-aed1-424e-9f23-b07533a52e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Spartial Analysis and Geographical pattern  \n",
    "\n",
    "def analyze_spatial_patterns(df):\n",
    "    \"\"\"Analyze spatial patterns in agricultural productivity\"\"\"\n",
    "    \n",
    "    # Calculate state-level statistics\n",
    "    state_stats = df.groupby('State Name').agg({\n",
    "        'OVERALL_YIELD_Kg_per_ha': ['mean', 'std', 'min', 'max'],\n",
    "        'TOTAL_PRODUCTION_1000_tons': 'sum',\n",
    "        'TOTAL_AREA_1000_ha': 'sum',\n",
    "        'CROP_DIVERSIFICATION_INDEX': 'mean',\n",
    "        'Dist Name': 'nunique'\n",
    "    }).round(2)\n",
    "    \n",
    "    state_stats.columns = ['_'.join(col).strip() for col in state_stats.columns.values]\n",
    "    state_stats = state_stats.rename(columns={'Dist Name_nunique': 'Number_of_Districts'})\n",
    "    \n",
    "    # Calculate productivity ranking\n",
    "    state_stats['Productivity_Rank'] = state_stats['OVERALL_YIELD_Kg_per_ha_mean'].rank(ascending=False)\n",
    "    state_stats['Stability_Score'] = 1 / (1 + state_stats['OVERALL_YIELD_Kg_per_ha_std'])\n",
    "    state_stats['Efficiency_Score'] = state_stats['TOTAL_PRODUCTION_1000_tons_sum'] / state_stats['TOTAL_AREA_1000_ha_sum']\n",
    "    \n",
    "    # Create visualization\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=('Average Yield by State', 'Total Production by State',\n",
    "                       'Crop Diversification Index', 'Agricultural Efficiency'),\n",
    "        specs=[[{'type': 'bar'}, {'type': 'bar'}],\n",
    "               [{'type': 'bar'}, {'type': 'bar'}]]\n",
    "    )\n",
    "    \n",
    "    # Sort states by yield\n",
    "    state_stats_sorted = state_stats.sort_values('OVERALL_YIELD_Kg_per_ha_mean', ascending=False)\n",
    "    \n",
    "    # Plot 1: Average Yield\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=state_stats_sorted.index, y=state_stats_sorted['OVERALL_YIELD_Kg_per_ha_mean'],\n",
    "               name='Avg Yield', marker_color='lightgreen'),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Plot 2: Total Production\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=state_stats_sorted.index, y=state_stats_sorted['TOTAL_PRODUCTION_1000_tons_sum'],\n",
    "               name='Total Production', marker_color='lightblue'),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # Plot 3: Crop Diversification\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=state_stats_sorted.index, y=state_stats_sorted['CROP_DIVERSIFICATION_INDEX_mean'],\n",
    "               name='Diversification', marker_color='orange'),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # Plot 4: Efficiency\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=state_stats_sorted.index, y=state_stats_sorted['Efficiency_Score'],\n",
    "               name='Efficiency', marker_color='red'), \n",
    "        row=2, col=2\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(height=800, showlegend=True, title_text=\"State-wise Agricultural Performance\")\n",
    "    fig.show()\n",
    "    \n",
    "    return state_stats\n",
    "\n",
    "state_stats = analyze_spatial_patterns(df)\n",
    "print(\"\\nTop 5 States by Average Yield:\")\n",
    "print(state_stats.sort_values('OVERALL_YIELD_Kg_per_ha_mean', ascending=False).head())\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4b088c-341a-44d5-9ed1-129a9995124a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_correlations(df):\n",
    "    \"\"\"Perform comprehensive correlation analysis\"\"\"\n",
    "    \n",
    "    # Select key numerical features\n",
    "    key_features = [\n",
    "        'OVERALL_YIELD_Kg_per_ha', 'TOTAL_PRODUCTION_1000_tons', 'TOTAL_AREA_1000_ha',\n",
    "        'CROP_DIVERSIFICATION_INDEX', 'PRODUCTIVITY_EFFICIENCY',\n",
    "        'RICE YIELD (Kg per ha)', 'WHEAT YIELD (Kg per ha)',\n",
    "        'SUGARCANE YIELD (Kg per ha)', 'COTTON YIELD (Kg per ha)'\n",
    "    ]\n",
    "    \n",
    "    # Filter available features\n",
    "    available_features = [f for f in key_features if f in df.columns]\n",
    "    \n",
    "    # Calculate correlation matrix\n",
    "    correlation_matrix = df[available_features].corr()\n",
    "    \n",
    "    # Create heatmap\n",
    "    fig = go.Figure(data=go.Heatmap(\n",
    "        z=correlation_matrix.values,\n",
    "        x=correlation_matrix.columns,\n",
    "        y=correlation_matrix.index,\n",
    "        colorscale='RdBu',\n",
    "        zmid=0,\n",
    "        text=correlation_matrix.round(2).values,\n",
    "        texttemplate='%{text}',\n",
    "        textfont={\"size\": 10},\n",
    "        hoverongaps=False\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title='Feature Correlation Matrix',\n",
    "        xaxis_title='Features',\n",
    "        yaxis_title='Features',\n",
    "        height=600,\n",
    "        width=800\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    # Identify strong correlations\n",
    "    strong_correlations = []\n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(i+1, len(correlation_matrix.columns)):\n",
    "            if abs(correlation_matrix.iloc[i, j]) > 0.7:\n",
    "                strong_correlations.append((\n",
    "                    correlation_matrix.columns[i],\n",
    "                    correlation_matrix.columns[j],\n",
    "                    correlation_matrix.iloc[i, j]\n",
    "                ))\n",
    "    \n",
    "    print(\"\\nStrong Correlations (|r| > 0.7):\")\n",
    "    print(\"=\" * 80)\n",
    "    for corr in strong_correlations:\n",
    "        print(f\"{corr[0]} ↔ {corr[1]}: {corr[2]:.3f}\")\n",
    "    \n",
    "    # Perform PCA for dimensionality reduction\n",
    "    print(\"\\nPrincipal Component Analysis:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Standardize data\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(df[available_features].fillna(0))\n",
    "    \n",
    "    # Apply PCA\n",
    "    pca = PCA()\n",
    "    pca_result = pca.fit_transform(scaled_data)\n",
    "    \n",
    "    # Plot explained variance\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    ax1.plot(range(1, len(pca.explained_variance_ratio_) + 1), \n",
    "             np.cumsum(pca.explained_variance_ratio_), 'bo-')\n",
    "    ax1.axhline(y=0.95, color='r', linestyle='--', label='95% Variance')\n",
    "    ax1.set_xlabel('Number of Components')\n",
    "    ax1.set_ylabel('Cumulative Explained Variance')\n",
    "    ax1.set_title('PCA Explained Variance')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Plot component loadings\n",
    "    components_to_show = min(3, len(available_features))\n",
    "    loadings = pca.components_[:components_to_show].T\n",
    "    \n",
    "    ax2.barh(range(len(available_features)), loadings[:, 0], label='PC1')\n",
    "    ax2.barh(range(len(available_features)), loadings[:, 1], left=loadings[:, 0], label='PC2')\n",
    "    ax2.set_yticks(range(len(available_features)))\n",
    "    ax2.set_yticklabels(available_features)\n",
    "    ax2.set_xlabel('Component Loadings')\n",
    "    ax2.set_title('PCA Component Loadings (First 2 Components)')\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nNumber of components for 95% variance: {np.where(np.cumsum(pca.explained_variance_ratio_) > 0.95)[0][0] + 1}\")\n",
    "    \n",
    "    return correlation_matrix, pca\n",
    "\n",
    "correlation_matrix, pca = analyze_correlations(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6c3491-df87-4538-aaf8-5f8bfb954038",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6 Clustering Analysis\n",
    "\n",
    "\n",
    "def perform_clustering_analysis(df, n_clusters=5):\n",
    "    \"\"\"Perform clustering analysis to identify similar districts\"\"\"\n",
    "    \n",
    "    # Prepare features for clustering\n",
    "    cluster_features = [\n",
    "        'OVERALL_YIELD_Kg_per_ha', 'TOTAL_PRODUCTION_1000_tons',\n",
    "        'TOTAL_AREA_1000_ha', 'CROP_DIVERSIFICATION_INDEX',\n",
    "        'PRODUCTIVITY_EFFICIENCY'\n",
    "    ]\n",
    "    \n",
    "    # Filter available features\n",
    "    available_cluster_features = [f for f in cluster_features if f in df.columns]\n",
    "    \n",
    "    # Get district-level averages\n",
    "    district_features = df.groupby(['State Name', 'Dist Name'])[available_cluster_features].mean().reset_index()\n",
    "    \n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    scaled_features = scaler.fit_transform(district_features[available_cluster_features])\n",
    "    \n",
    "    # Apply K-means clustering\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    district_features['Cluster'] = kmeans.fit_predict(scaled_features)\n",
    "    \n",
    "    # Analyze cluster characteristics\n",
    "    cluster_stats = district_features.groupby('Cluster')[available_cluster_features].agg(['mean', 'std'])\n",
    "    \n",
    "    # Create visualization\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=('Cluster Distribution', 'Yield vs Diversification',\n",
    "                       'Production vs Area', 'Cluster Characteristics'),\n",
    "        specs=[[{'type': 'pie'}, {'type': 'scatter'}],\n",
    "               [{'type': 'scatter'}, {'type': 'bar'}]]\n",
    "    )\n",
    "    \n",
    "    # Pie chart for cluster distribution\n",
    "    cluster_counts = district_features['Cluster'].value_counts().sort_index()\n",
    "    fig.add_trace(\n",
    "        go.Pie(labels=[f'Cluster {i}' for i in cluster_counts.index],\n",
    "               values=cluster_counts.values, name='Cluster Distribution'),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Scatter plot: Yield vs Diversification\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=district_features['OVERALL_YIELD_Kg_per_ha'],\n",
    "                   y=district_features['CROP_DIVERSIFICATION_INDEX'],\n",
    "                   mode='markers',\n",
    "                   marker=dict(color=district_features['Cluster'], colorscale='viridis', showscale=True),\n",
    "                   text=district_features['Dist Name'],\n",
    "                   hoverinfo='text+x+y',\n",
    "                   name='Districts'),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    fig.update_xaxes(title_text='Overall Yield', row=1, col=2)\n",
    "    fig.update_yaxes(title_text='Diversification Index', row=1, col=2)\n",
    "    \n",
    "    # Scatter plot: Production vs Area\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=district_features['TOTAL_AREA_1000_ha'],\n",
    "                   y=district_features['TOTAL_PRODUCTION_1000_tons'],\n",
    "                   mode='markers',\n",
    "                   marker=dict(color=district_features['Cluster'], colorscale='plasma', showscale=False),\n",
    "                   text=district_features['Dist Name'],\n",
    "                   hoverinfo='text+x+y',\n",
    "                   name='Districts'),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    fig.update_xaxes(title_text='Total Area', row=2, col=1)\n",
    "    fig.update_yaxes(title_text='Total Production', row=2, col=1)\n",
    "    \n",
    "    # Bar chart for cluster means\n",
    "    for i, feature in enumerate(['OVERALL_YIELD_Kg_per_ha', 'TOTAL_PRODUCTION_1000_tons']):\n",
    "        if feature in available_cluster_features:\n",
    "            fig.add_trace(\n",
    "                go.Bar(x=[f'Cluster {j}' for j in range(n_clusters)],\n",
    "                       y=cluster_stats[(feature, 'mean')],\n",
    "                       name=feature,\n",
    "                       showlegend=(i==0)),\n",
    "                row=2, col=2\n",
    "            )\n",
    "    \n",
    "    fig.update_layout(height=800, showlegend=True, title_text=\"District Clustering Analysis\")\n",
    "    fig.show()\n",
    "    \n",
    "    # Print cluster insights\n",
    "    print(\"\\nCluster Characteristics:\")\n",
    "    print(\"=\" * 80)\n",
    "    for cluster_id in range(n_clusters):\n",
    "        cluster_data = district_features[district_features['Cluster'] == cluster_id]\n",
    "        print(f\"\\nCluster {cluster_id} ({len(cluster_data)} districts):\")\n",
    "        print(f\"  Average Yield: {cluster_data['OVERALL_YIELD_Kg_per_ha'].mean():.2f}\")\n",
    "        print(f\"  Average Production: {cluster_data['TOTAL_PRODUCTION_1000_tons'].mean():.2f}\")\n",
    "        print(f\"  Average Diversification: {cluster_data['CROP_DIVERSIFICATION_INDEX'].mean():.3f}\")\n",
    "        print(f\"  States: {', '.join(cluster_data['State Name'].unique()[:5])}\")\n",
    "    \n",
    "    return district_features, kmeans\n",
    "\n",
    "district_clusters, kmeans_model = perform_clustering_analysis(df, n_clusters=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1318fb1d-11c2-4e26-883e-49e0815f8d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7. Anomaly Detection\n",
    "\n",
    "def detect_anomalies(df):\n",
    "    \"\"\"Detect anomalies and outliers in the data\"\"\"\n",
    "    \n",
    "    # Method 1: Statistical approach using Z-score\n",
    "    print(\"Statistical Anomaly Detection (Z-score > 3):\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    anomaly_cols = [\n",
    "        'OVERALL_YIELD_Kg_per_ha', \n",
    "        'TOTAL_PRODUCTION_1000_tons',\n",
    "        'RICE YIELD (Kg per ha)', \n",
    "        'WHEAT YIELD (Kg per ha)'\n",
    "    ]\n",
    "    \n",
    "    anomalies = {}\n",
    "    \n",
    "    for col in anomaly_cols:\n",
    "        if col in df.columns:\n",
    "\n",
    "            # SAFE & FIXED Z-SCORE CALCULATION  \n",
    "            col_data = df[col]\n",
    "\n",
    "            # Compute z-scores without breaking index alignment\n",
    "            z_scores = stats.zscore(col_data, nan_policy='omit')\n",
    "\n",
    "            # Convert to absolute values and handle NaNs safely\n",
    "            z_scores = np.abs(z_scores)\n",
    "\n",
    "            # Mask automatically aligns with df rows because we never dropped anything\n",
    "            mask = (z_scores > 3)\n",
    "\n",
    "            # Filter anomalies safely\n",
    "            col_anomalies = df[mask]\n",
    "            anomalies[col] = col_anomalies\n",
    "\n",
    "            print(f\"\\n{col}:\")\n",
    "            print(f\"  Number of anomalies: {len(col_anomalies)}\")\n",
    "            print(f\"  Anomaly percentage: {len(col_anomalies)/len(df)*100:.2f}%\")\n",
    "            \n",
    "            if len(col_anomalies) > 0:\n",
    "                print(\n",
    "                    f\"  Sample anomalies: \"\n",
    "                    f\"{col_anomalies[['State Name', 'Dist Name', 'Year', col]].head(3).to_string(index=False)}\"\n",
    "                )\n",
    "    \n",
    "    # Method 2: Isolation Forest\n",
    "    from sklearn.ensemble import IsolationForest\n",
    "    \n",
    "    iso_features = [\n",
    "        'OVERALL_YIELD_Kg_per_ha', \n",
    "        'TOTAL_PRODUCTION_1000_tons',\n",
    "        'TOTAL_AREA_1000_ha', \n",
    "        'CROP_DIVERSIFICATION_INDEX'\n",
    "    ]\n",
    "    iso_features = [f for f in iso_features if f in df.columns]\n",
    "    \n",
    "    iso_data = df[iso_features].fillna(0)\n",
    "    \n",
    "    iso_forest = IsolationForest(contamination=0.05, random_state=42)\n",
    "    iso_predictions = iso_forest.fit_predict(iso_data)\n",
    "    \n",
    "    df['Isolation_Forest_Anomaly'] = iso_predictions\n",
    "    iso_anomalies = df[df['Isolation_Forest_Anomaly'] == -1]\n",
    "    \n",
    "    print(f\"\\nIsolation Forest Anomalies: {len(iso_anomalies)} ({len(iso_anomalies)/len(df)*100:.2f}%)\")\n",
    "    \n",
    "    # Visualize anomalies\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Normal points\n",
    "    normal = df[df['Isolation_Forest_Anomaly'] == 1]\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=normal['TOTAL_AREA_1000_ha'],\n",
    "        y=normal['OVERALL_YIELD_Kg_per_ha'],\n",
    "        mode='markers',\n",
    "        name='Normal',\n",
    "        marker=dict(color='blue', size=8, opacity=0.6)\n",
    "    ))\n",
    "    \n",
    "    # Anomalies\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=iso_anomalies['TOTAL_AREA_1000_ha'],\n",
    "        y=iso_anomalies['OVERALL_YIELD_Kg_per_ha'],\n",
    "        mode='markers',\n",
    "        name='Anomaly',\n",
    "        marker=dict(color='red', size=12, symbol='x')\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title='Anomaly Detection using Isolation Forest',\n",
    "        xaxis_title='Total Area (1000 ha)',\n",
    "        yaxis_title='Overall Yield (Kg/ha)',\n",
    "        height=600\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    return anomalies, iso_anomalies\n",
    "\n",
    "anomalies, iso_anomalies = detect_anomalies(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a868950-0eb9-4cd4-b5c7-c2b49081361d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Crop Diversification Analysis\n",
    "\n",
    "def analyze_crop_diversification(df):\n",
    "    \"\"\"Analyze crop diversification patterns\"\"\"\n",
    "    \n",
    "    # Calculate Herfindahl-Hirschman Index (HHI) for each district-year\n",
    "    area_columns = [col for col in df.columns if 'AREA' in col and '1000 ha' in col]\n",
    "    \n",
    "    diversification_analysis = []\n",
    "    for idx, row in df.iterrows():\n",
    "        areas = row[area_columns]\n",
    "        total_area = areas.sum()\n",
    "        \n",
    "        if total_area > 0:\n",
    "            proportions = areas / total_area\n",
    "\n",
    "            # ---- SAFEST FIX: Convert to numpy array (prevents .log error) ----\n",
    "            proportions_np = proportions.to_numpy(dtype=float)\n",
    "\n",
    "            # HHI\n",
    "            hhi = np.sum(proportions_np ** 2)\n",
    "\n",
    "            # Entropy (fully safe numeric operations)\n",
    "            entropy = -np.sum(proportions_np * np.log(proportions_np + 1e-10))\n",
    "            # -------------------------------------------------------------------\n",
    "\n",
    "            diversification_analysis.append({\n",
    "                'State': row['State Name'],\n",
    "                'District': row['Dist Name'],\n",
    "                'Year': row['Year'],\n",
    "                'HHI': hhi,\n",
    "                'Entropy': entropy,\n",
    "                'Total_Crops': (areas > 0).sum(),\n",
    "                'Dominant_Crop': areas.idxmax() if not areas.empty else None\n",
    "            })\n",
    "    \n",
    "    diversification_df = pd.DataFrame(diversification_analysis)\n",
    "    \n",
    "    # Analyze trends\n",
    "    print(\"Crop Diversification Analysis:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Yearly trends\n",
    "    yearly_diversification = diversification_df.groupby('Year').agg({\n",
    "        'HHI': 'mean',\n",
    "        'Entropy': 'mean',\n",
    "        'Total_Crops': 'mean'\n",
    "    }).round(3)\n",
    "    \n",
    "    print(\"\\nYearly Diversification Trends:\")\n",
    "    print(yearly_diversification)\n",
    "    \n",
    "    # State-level comparison\n",
    "    state_diversification = diversification_df.groupby('State').agg({\n",
    "        'HHI': ['mean', 'std'],\n",
    "        'Entropy': ['mean', 'std'],\n",
    "        'Total_Crops': 'mean'\n",
    "    }).round(3)\n",
    "    \n",
    "    state_diversification.columns = ['_'.join(col).strip() for col in state_diversification.columns.values]\n",
    "    state_diversification = state_diversification.sort_values('HHI_mean')\n",
    "    \n",
    "    print(\"\\nState-level Diversification (sorted by HHI - lower is more diversified):\")\n",
    "    print(state_diversification.head(10))\n",
    "    \n",
    "    # Visualization\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=('HHI Trend Over Time', 'Entropy Trend Over Time',\n",
    "                       'State-wise HHI Comparison', 'Diversification vs Yield'),\n",
    "        vertical_spacing=0.15\n",
    "    )\n",
    "    \n",
    "    # HHI trend\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=yearly_diversification.index, y=yearly_diversification['HHI'],\n",
    "                  mode='lines+markers', name='HHI'),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Entropy trend\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=yearly_diversification.index, y=yearly_diversification['Entropy'],\n",
    "                  mode='lines+markers', name='Entropy', line=dict(color='red')),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # State HHI comparison\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=state_diversification.index, y=state_diversification['HHI_mean'],\n",
    "               name='HHI by State'),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # Diversification vs Yield\n",
    "    merged_data = pd.merge(\n",
    "        diversification_df,\n",
    "        df[['State Name', 'Dist Name', 'Year', 'OVERALL_YIELD_Kg_per_ha']],\n",
    "        left_on=['State', 'District', 'Year'],\n",
    "        right_on=['State Name', 'Dist Name', 'Year']\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=merged_data['HHI'],\n",
    "            y=merged_data['OVERALL_YIELD_Kg_per_ha'],\n",
    "            mode='markers',\n",
    "            name='HHI vs Yield',\n",
    "            marker=dict(size=8, opacity=0.6, color=merged_data['Year'],\n",
    "                        colorscale='viridis', showscale=True)\n",
    "        ),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(height=800, showlegend=True, title_text=\"Crop Diversification Analysis\")\n",
    "    fig.update_xaxes(title_text=\"Year\", row=1, col=1)\n",
    "    fig.update_xaxes(title_text=\"Year\", row=1, col=2)\n",
    "    fig.update_xaxes(title_text=\"State\", row=2, col=1)\n",
    "    fig.update_xaxes(title_text=\"HHI (Lower = More Diversified)\", row=2, col=2)\n",
    "    fig.update_yaxes(title_text=\"HHI Index\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Entropy\", row=1, col=2)\n",
    "    fig.update_yaxes(title_text=\"HHI\", row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"Yield (Kg/ha)\", row=2, col=2)\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    return diversification_df\n",
    "\n",
    "diversification_df = analyze_crop_diversification(df)   \n",
    "   \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b15084-6615-4991-8327-db2c66fcd07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Advanced Statistical Analysis\n",
    "\n",
    "def advanced_statistical_analysis(df):\n",
    "    \"\"\"Perform advanced statistical analyses\"\"\"\n",
    "    \n",
    "    print(\"Advanced Statistical Analysis\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # 1. Stationarity test for time series\n",
    "    print(\"\\n1. Time Series Stationarity Tests:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Prepare time series data (state-level average yield)\n",
    "    time_series_data = df.groupby('Year')['OVERALL_YIELD_Kg_per_ha'].mean()\n",
    "    \n",
    "    # ADF test\n",
    "    adf_result = adfuller(time_series_data)\n",
    "    print(f\"ADF Test:\")\n",
    "    print(f\"  ADF Statistic: {adf_result[0]:.4f}\")\n",
    "    print(f\"  p-value: {adf_result[1]:.4f}\")\n",
    "    print(f\"  Critical Values: {adf_result[4]}\")\n",
    "    \n",
    "    # KPSS test\n",
    "    kpss_result = kpss(time_series_data, regression='c')\n",
    "    print(f\"\\nKPSS Test:\")\n",
    "    print(f\"  KPSS Statistic: {kpss_result[0]:.4f}\")\n",
    "    print(f\"  p-value: {kpss_result[1]:.4f}\")\n",
    "    \n",
    "    # 2. Regression analysis\n",
    "    print(\"\\n2. Regression Analysis - Factors affecting yield:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Prepare regression data\n",
    "    reg_features = ['TOTAL_AREA_1000_ha', 'CROP_DIVERSIFICATION_INDEX', \n",
    "                    'YEAR_SINCE_2010', 'PRODUCTIVITY_EFFICIENCY']\n",
    "    reg_features = [f for f in reg_features if f in df.columns]\n",
    "    \n",
    "    X = df[reg_features].fillna(0)\n",
    "    y = df['OVERALL_YIELD_Kg_per_ha']\n",
    "    \n",
    "    # Add constant\n",
    "    X = sm.add_constant(X)\n",
    "    \n",
    "    # Fit OLS model\n",
    "    model = sm.OLS(y, X).fit()\n",
    "    \n",
    "    print(model.summary().tables[1])\n",
    "    \n",
    "    # 3. Hypothesis testing\n",
    "    print(\"\\n3. Hypothesis Testing:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Compare yields between states\n",
    "    states_to_compare = df['State Name'].value_counts().index[:2]\n",
    "    state1_yield = df[df['State Name'] == states_to_compare[0]]['OVERALL_YIELD_Kg_per_ha']\n",
    "    state2_yield = df[df['State Name'] == states_to_compare[1]]['OVERALL_YIELD_Kg_per_ha']\n",
    "    \n",
    "    # T-test\n",
    "    t_stat, p_value = stats.ttest_ind(state1_yield, state2_yield, equal_var=False)\n",
    "    print(f\"T-test: {states_to_compare[0]} vs {states_to_compare[1]}\")\n",
    "    print(f\"  t-statistic: {t_stat:.4f}\")\n",
    "    print(f\"  p-value: {p_value:.4f}\")\n",
    "    \n",
    "    if p_value < 0.05:\n",
    "        print(f\"  → Statistically significant difference in yields\")\n",
    "    else:\n",
    "        print(f\"  → No significant difference in yields\")\n",
    "    \n",
    "    # 4. ANOVA for multiple groups\n",
    "    print(\"\\n4. ANOVA - Yield across states:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Select top 5 states\n",
    "    top_states = df['State Name'].value_counts().index[:5]\n",
    "    anova_data = [df[df['State Name'] == state]['OVERALL_YIELD_Kg_per_ha'] for state in top_states]\n",
    "    \n",
    "    f_stat, p_value = stats.f_oneway(*anova_data)\n",
    "    print(f\"ANOVA Results:\")\n",
    "    print(f\"  F-statistic: {f_stat:.4f}\")\n",
    "    print(f\"  p-value: {p_value:.4f}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "regression_model = advanced_statistical_analysis(df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284a11e4-87d0-4fc9-81f9-b636c818dde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Save Analysis Results\n",
    "\n",
    "\n",
    "# Save all analysis results\n",
    "analysis_results = {\n",
    "    'state_statistics': state_stats,\n",
    "    'correlation_matrix': correlation_matrix,\n",
    "    'district_clusters': district_clusters,\n",
    "    'kmeans_model': kmeans_model,\n",
    "    'anomalies': anomalies,\n",
    "    'diversification_analysis': diversification_df,\n",
    "    'regression_model_summary': regression_model.summary(),\n",
    "    'pca_explained_variance': pca.explained_variance_ratio_,\n",
    "    'analysis_timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "}\n",
    "\n",
    "# Save to pickle\n",
    "with open('eda_analysis_results.pkl', 'wb') as f:\n",
    "    pickle.dump(analysis_results, f)\n",
    "\n",
    "# Save key visualizations\n",
    "figures_to_save = ['missing_values_matrix.png']\n",
    "for fig_name in figures_to_save:\n",
    "    if os.path.exists(fig_name):\n",
    "        print(f\"Saved: {fig_name}\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"EXPLORATORY DATA ANALYSIS COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Analysis completed at: {datetime.now()}\")\n",
    "print(f\"Results saved to: eda_analysis_results.pkl\")\n",
    "print(f\"\\nKey Insights Generated:\")\n",
    "print(\"  1. Temporal trend analysis with structural break detection\")\n",
    "print(\"  2. Spatial patterns and state rankings\")\n",
    "print(\"  3. Correlation analysis and PCA\")\n",
    "print(\"  4. District clustering (5 clusters identified)\")\n",
    "print(\"  5. Anomaly detection using multiple methods\")\n",
    "print(\"  6. Crop diversification analysis with HHI and entropy\")\n",
    "print(\"  7. Advanced statistical tests and regression analysis\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45eaa4d1-b215-43cb-80dd-b99cba23f2b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
