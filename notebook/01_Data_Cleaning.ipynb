{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206edc96-4a35-44c8-a149-7eb40e0d380d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook handles all data cleaning, validation, and preprocessing steps for the Indian agricultural dataset.\n",
    "\n",
    "#1. Import libraries and setup \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from datetime import datetime\n",
    "import missingno as msno\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "from scipy import stats \n",
    "import pickle \n",
    "import os \n",
    "\n",
    "\n",
    "\n",
    "#Set Display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(f\"Analysis started at : {datetime.now()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c4ac1e-a883-46b7-a8fd-ff30d5402967",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Load and Inspect Raw Data \n",
    "\n",
    "df =  pd.read_csv('../data/Crops_data.csv')\n",
    "\n",
    "print(f\"Dataset Shape:  {df.shape}\")\n",
    "print(f\"Columns:  {len(df.columns)}\")\n",
    "print(f\"Years Covered: {df['Year'].unique()}\")\n",
    "print(f\"States covered: {df['State Name'].unique()}\")\n",
    "print(f\"Number of districts:  {df['Dist Name'].nunique()}\")\n",
    "\n",
    "# Display basic information \n",
    "\n",
    "print(\"Dataset Information:\")\n",
    "print(\"=\" * 80 )\n",
    "print(df.info())\n",
    "\n",
    "\n",
    "# Check for Duplicates \n",
    "print(f\"\\nDuplicated rows: {df.duplicated().sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2a83c2-5372-4c12-8fe0-ba79a715726c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  3 Data Quality Assessment \n",
    "\n",
    "def assess_data_quality(df):\n",
    "    \"\"\"Comprehensive data quality assessment\"\"\"\n",
    "    \n",
    "    quality_report = pd.DataFrame({\n",
    "        'Column': df.columns,\n",
    "        'Data_Type': df.dtypes.values,\n",
    "        'Non_Null_Count': df.count().values,\n",
    "        'Null_Count': df.isnull().sum().values,\n",
    "        'Null_Percentage': (df.isnull().sum() / len(df) * 100).values,\n",
    "        'Unique_Values': df.nunique().values,\n",
    "        'Sample_Values': [df[col].dropna().unique()[:3] for col in df.columns]\n",
    "    })\n",
    "    \n",
    "    # Add statistical summaries for numeric columns\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_cols:\n",
    "        quality_report.loc[quality_report['Column'] == col, 'Min'] = df[col].min()\n",
    "        quality_report.loc[quality_report['Column'] == col, 'Max'] = df[col].max()\n",
    "        quality_report.loc[quality_report['Column'] == col, 'Mean'] = df[col].mean()\n",
    "        quality_report.loc[quality_report['Column'] == col, 'Std'] = df[col].std()\n",
    "    \n",
    "    return quality_report\n",
    "\n",
    "\n",
    "# Generate quality report\n",
    "quality_report = assess_data_quality(df)\n",
    "print(\"\\nData Quality Report:\")\n",
    "print(\"=\" * 80)\n",
    "print(quality_report[['Column', 'Data_Type', 'Null_Percentage', 'Unique_Values']].head(20))\n",
    "\n",
    "# %%\n",
    "# Visualize missing values\n",
    "plt.figure(figsize=(16, 8))\n",
    "msno.matrix(df)\n",
    "plt.title('Missing Values Matrix', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('missing_values_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96673fb8-cb0a-4490-91bc-ca9e23805595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Handle Missing Values and Data Imputation\n",
    "\n",
    "# %%\n",
    "def handle_missing_values(df):\n",
    "    \"\"\"Advanced missing value handling with multiple strategies\"\"\"\n",
    "    \n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Separate categorical and numerical columns\n",
    "    categorical_cols = df_clean.select_dtypes(include=['object']).columns\n",
    "    numerical_cols = df_clean.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    # Handle categorical missing values\n",
    "    for col in categorical_cols:\n",
    "        if df_clean[col].isnull().sum() > 0:\n",
    "            df_clean[col] = df_clean[col].fillna('Unknown')\n",
    "    \n",
    "    # Handle numerical missing values using multiple strategies\n",
    "    for col in numerical_cols:\n",
    "        null_count = df_clean[col].isnull().sum()\n",
    "        \n",
    "        if null_count > 0:\n",
    "            print(f\"Column: {col} - Missing: {null_count} ({null_count/len(df)*100:.2f}%)\")\n",
    "            \n",
    "            # Strategy 1: Use forward fill for time-series data within each district\n",
    "            if col in ['RICE AREA (1000 ha)', 'RICE PRODUCTION (1000 tons)', 'RICE YIELD (Kg per ha)']:\n",
    "                df_clean[col] = df_clean.groupby(['State Name', 'Dist Name'])[col].ffill()\n",
    "            \n",
    "            # Strategy 2: Use median by state and year for remaining\n",
    "            remaining_nulls = df_clean[col].isnull().sum()\n",
    "            if remaining_nulls > 0:\n",
    "                df_clean[col] = df_clean.groupby(['State Name', 'Year'])[col].transform(\n",
    "                    lambda x: x.fillna(x.median())\n",
    "                )\n",
    "            \n",
    "            # Strategy 3: Use overall median for any remaining\n",
    "            df_clean[col] = df_clean[col].fillna(df_clean[col].median())\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "# Apply missing value handling\n",
    "df_clean = handle_missing_values(df)\n",
    "\n",
    "# Verify missing values are handled\n",
    "print(f\"\\nRemaining missing values: {df_clean.isnull().sum().sum()}\")\n",
    "\n",
    "# %% [markdown]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de78c5b3-c188-442a-875a-380508903d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. Data Type Conversion and Validation\n",
    "\n",
    "# %%\n",
    "def convert_data_types(df):\n",
    "    \"\"\"Ensure correct data types\"\"\"\n",
    "    \n",
    "    df_converted = df.copy()\n",
    "    \n",
    "    # Convert categorical columns\n",
    "    categorical_columns = ['Dist Name', 'State Name']\n",
    "    for col in categorical_columns:\n",
    "        df_converted[col] = df_converted[col].astype('category')\n",
    "    \n",
    "    # Convert ID columns to integers\n",
    "    id_columns = ['Dist Code', 'State Code', 'Year']\n",
    "    for col in id_columns:\n",
    "        df_converted[col] = pd.to_numeric(df_converted[col], errors='coerce').astype('int64')\n",
    "    \n",
    "    return df_converted\n",
    "\n",
    "df_clean = convert_data_types(df_clean)\n",
    "\n",
    "# %%\n",
    "# Validate data types\n",
    "print(\"Data Types after Conversion:\")\n",
    "print(df_clean.dtypes.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534526d8-f33e-45c2-9e04-10ed176c02f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Outlier Detection and Treatment\n",
    "\n",
    "def detect_outliers_iqr(df, column):\n",
    "    \"\"\"Detect outliers using IQR method \"\"\"\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR  = Q3 -Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n",
    "    return outliers,lower_bound ,upper_bound\n",
    "\n",
    "\n",
    "def visualize_outliers(df,column):\n",
    "    \"\"\"Vizualize outliers for a specific column\"\"\"\n",
    "    fig , axes = plt.subplots(1,2, figsize =(15,6))\n",
    "\n",
    "\n",
    "    # Box plot \n",
    "\n",
    "    axes[0].boxplot(df[column].dropna())\n",
    "    axes[0].set_title(f\"Box Plot of {column}\")\n",
    "    axes[0].set_ylabel('Values')\n",
    "\n",
    "\n",
    "\n",
    "    #Histogram with outliers highlighted \n",
    "\n",
    "\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    outliers = df[(df[column] < Q1 - 1.5*IQR) | (df[column] > Q3 + 1.5*IQR)][column]\n",
    "    \n",
    "    axes[1].hist(df[column].dropna(), bins=50, alpha=0.7, label='Normal')\n",
    "    axes[1].hist(outliers, bins=50, alpha=0.7, color='red', label='Outliers')\n",
    "    axes[1].set_title(f'Histogram of {column} with Outliers')\n",
    "    axes[1].set_xlabel('Values')\n",
    "    axes[1].set_ylabel('Frequency')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Check key columns for outliers\n",
    "key_columns = ['RICE YIELD (Kg per ha)', 'WHEAT YIELD (Kg per ha)', \n",
    "               'SUGARCANE YIELD (Kg per ha)', 'COTTON YIELD (Kg per ha)']\n",
    "\n",
    "for col in key_columns:\n",
    "    if col in df_clean.columns:\n",
    "        outliers, lower, upper = detect_outliers_iqr(df_clean, col)\n",
    "        print(f\"\\n{col}:\")\n",
    "        print(f\"  Lower bound: {lower:.2f}, Upper bound: {upper:.2f}\")\n",
    "        print(f\"  Number of outliers: {len(outliers)}\")\n",
    "        print(f\"  Outlier percentage: {len(outliers)/len(df_clean)*100:.2f}%\")\n",
    "        \n",
    "        if len(outliers) > 0:\n",
    "            visualize_outliers(df_clean, col)\n",
    "\n",
    "\n",
    "def handle_outliers(df, method='winsorize'):\n",
    "    \"\"\"Handle outliers using specified method\"\"\"\n",
    "    \n",
    "    df_processed = df.copy()\n",
    "    numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    # Remove ID columns from outlier treatment\n",
    "    id_cols = ['Dist Code', 'State Code', 'Year']\n",
    "    numerical_cols = [col for col in numerical_cols if col not in id_cols]\n",
    "    \n",
    "    if method == 'winsorize':\n",
    "        # Winsorize outliers (cap at 1st and 99th percentiles)\n",
    "        for col in numerical_cols:\n",
    "            lower_limit = df_processed[col].quantile(0.01)\n",
    "            upper_limit = df_processed[col].quantile(0.99)\n",
    "            df_processed[col] = df_processed[col].clip(lower=lower_limit, upper=upper_limit)\n",
    "    \n",
    "    elif method == 'iqr':\n",
    "        # Cap outliers using IQR method\n",
    "        for col in numerical_cols:\n",
    "            Q1 = df_processed[col].quantile(0.25)\n",
    "            Q3 = df_processed[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "            \n",
    "            df_processed[col] = df_processed[col].clip(lower=lower_bound, upper=upper_bound)\n",
    "    \n",
    "    return df_processed\n",
    "\n",
    "    # Apply outlier handling\n",
    "df_clean = handle_outliers(df_clean, method='winsorize')\n",
    "\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f87c70-f905-4094-af65-2b6354bd089e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Feature Engineering :\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_derived_features(df):\n",
    "    \"\"\"Create meaningful derived features\"\"\"\n",
    "    \n",
    "    df_enhanced = df.copy()\n",
    "    \n",
    "    # 1. Calculate total agricultural area\n",
    "    area_columns = [col for col in df.columns if 'AREA' in col and '1000 ha' in col]\n",
    "    df_enhanced['TOTAL_AREA_1000_ha'] = df_enhanced[area_columns].sum(axis=1, min_count=1)\n",
    "    \n",
    "    # 2. Calculate total production\n",
    "    production_columns = [col for col in df.columns if 'PRODUCTION' in col and '1000 tons' in col]\n",
    "    df_enhanced['TOTAL_PRODUCTION_1000_tons'] = df_enhanced[production_columns].sum(axis=1, min_count=1)\n",
    "    \n",
    "    # 3. Calculate overall yield (weighted average)\n",
    "    df_enhanced['OVERALL_YIELD_Kg_per_ha'] = df_enhanced['TOTAL_PRODUCTION_1000_tons'] * 1000 / df_enhanced['TOTAL_AREA_1000_ha']\n",
    "    \n",
    "    # 4. Calculate crop diversification index (Herfindahl-Hirschman Index)\n",
    "    area_proportion_columns = []\n",
    "    for area_col in area_columns:\n",
    "        prop_col = f'{area_col}_PROPORTION'\n",
    "        df_enhanced[prop_col] = df_enhanced[area_col] / df_enhanced['TOTAL_AREA_1000_ha']\n",
    "        area_proportion_columns.append(prop_col)\n",
    "    \n",
    "    df_enhanced['CROP_DIVERSIFICATION_INDEX'] = (df_enhanced[area_proportion_columns] ** 2).sum(axis=1)\n",
    "    \n",
    "    # 5. Create productivity efficiency metrics\n",
    "    df_enhanced['PRODUCTIVITY_EFFICIENCY'] = df_enhanced['OVERALL_YIELD_Kg_per_ha'] / df_enhanced['OVERALL_YIELD_Kg_per_ha'].max()\n",
    "    \n",
    "    # 6. Create year-based features\n",
    "    df_enhanced['YEAR_SINCE_2010'] = df_enhanced['Year'] - 2010\n",
    "    df_enhanced['IS_MONSOON_YEAR'] = ((df_enhanced['Year'] % 4) == 0).astype(int)  # Simple proxy\n",
    "    \n",
    "    # 7. Calculate year-over-year growth rates\n",
    "    df_enhanced = df_enhanced.sort_values(['State Name', 'Dist Name', 'Year'])\n",
    "    \n",
    "    for col in ['TOTAL_PRODUCTION_1000_tons', 'OVERALL_YIELD_Kg_per_ha']:\n",
    "        df_enhanced[f'{col}_YoY_Growth'] = df_enhanced.groupby(['State Name', 'Dist Name'])[col].pct_change()\n",
    "    \n",
    "    return df_enhanced\n",
    "\n",
    "df_enhanced = create_derived_features(df_clean)\n",
    "\n",
    "\n",
    "# Display new features\n",
    "print(\"New Derived Features Created:\")\n",
    "new_features = [col for col in df_enhanced.columns if col not in df.columns]\n",
    "for feature in new_features:\n",
    "    print(f\"  - {feature}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a8c454-582a-4bdd-b273-375ef58db78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Data validation and Quality Checks \n",
    "\n",
    "\n",
    "def perform_data_validation(df):\n",
    "    \"\"\"Perform comprehensive data validation\"\"\"\n",
    "    \n",
    "    validation_results = {}\n",
    "    \n",
    "    # 1. Check for logical consistency\n",
    "    validation_results['yield_consistency'] = {}\n",
    "    yield_columns = [col for col in df.columns if 'YIELD' in col]\n",
    "    \n",
    "    for col in yield_columns:\n",
    "        # Yield should be positive\n",
    "        negative_yields = df[df[col] < 0]\n",
    "        validation_results['yield_consistency'][col] = {\n",
    "            'negative_count': len(negative_yields),\n",
    "            'percentage': len(negative_yields) / len(df) * 100\n",
    "        }\n",
    "    \n",
    "    # 2. Check area-production-yield relationship\n",
    "    validation_results['area_prod_yield'] = {}\n",
    "    crops = ['RICE', 'WHEAT', 'SUGARCANE']\n",
    "    \n",
    "    for crop in crops:\n",
    "        area_col = f'{crop} AREA (1000 ha)'\n",
    "        prod_col = f'{crop} PRODUCTION (1000 tons)'\n",
    "        yield_col = f'{crop} YIELD (Kg per ha)'\n",
    "        \n",
    "        if all(col in df.columns for col in [area_col, prod_col, yield_col]):\n",
    "            # Calculate expected yield\n",
    "            expected_yield = (df[prod_col] * 1000) / df[area_col]\n",
    "            \n",
    "            # Check consistency (allow 10% tolerance)\n",
    "            tolerance = 0.1\n",
    "            inconsistent = df[abs(expected_yield - df[yield_col]) / df[yield_col] > tolerance]\n",
    "            \n",
    "            validation_results['area_prod_yield'][crop] = {\n",
    "                'inconsistent_count': len(inconsistent),\n",
    "                'percentage': len(inconsistent) / len(df) * 100\n",
    "            }\n",
    "    \n",
    "    return validation_results\n",
    "\n",
    "# Run validation\n",
    "validation_results = perform_data_validation(df_enhanced)\n",
    "print(\"Data Validation Results:\")\n",
    "print(\"=\" * 80)\n",
    "for check_type, results in validation_results.items():\n",
    "    print(f\"\\n{check_type.upper()}:\")\n",
    "    for key, value in results.items():\n",
    "        print(f\"  {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01b87be-3e2b-4fb2-949b-c4b63d1015b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 9. Save Cleaned Dataset \n",
    "\n",
    " \n",
    "\n",
    "# Save CSV normally\n",
    "df_enhanced.to_csv('cleaned_agricultural_data.csv', index=False)\n",
    "\n",
    "# Save Parquet using fastparquet instead of pyarrow\n",
    "df_enhanced.to_parquet(\n",
    "    'cleaned_agricultural_data.parquet',\n",
    "    index=False,\n",
    "    engine='fastparquet'\n",
    ")\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    'cleaning_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'original_shape': df.shape,\n",
    "    'cleaned_shape': df_enhanced.shape,\n",
    "    'missing_values_original': df.isnull().sum().sum(),\n",
    "    'missing_values_cleaned': df_enhanced.isnull().sum().sum(),\n",
    "    'new_features_created': len([col for col in df_enhanced.columns if col not in df.columns]),\n",
    "    'validation_results': validation_results\n",
    "}\n",
    "\n",
    "with open('data_cleaning_metadata.pkl', 'wb') as f:\n",
    "    pickle.dump(metadata, f)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DATA CLEANING COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Original dataset shape: {df.shape}\")\n",
    "print(f\"Cleaned dataset shape: {df_enhanced.shape}\")\n",
    "print(\"Files saved:\")\n",
    "print(\"  - cleaned_agricultural_data.csv\")\n",
    "print(\"  - cleaned_agricultural_data.parquet (fastparquet engine)\")\n",
    "print(\"  - data_cleaning_metadata.pkl\")\n",
    "print(f\"\\nCleaning completed at: {datetime.now()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f34d6f-170a-42b7-aef4-3f0c77fb162b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
