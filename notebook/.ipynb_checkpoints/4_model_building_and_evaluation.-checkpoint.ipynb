{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e210fdaa-880d-41e6-95cd-773be5e660db",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'interp' from 'scipy' (C:\\Users\\phill\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\scipy\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 31\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (mean_squared_error, mean_absolute_error, r2_score,\n\u001b[0;32m     27\u001b[0m                           accuracy_score, precision_score, recall_score, f1_score,\n\u001b[0;32m     28\u001b[0m                           roc_auc_score, confusion_matrix, classification_report,\n\u001b[0;32m     29\u001b[0m                           mean_absolute_percentage_error, explained_variance_score)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minspection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m permutation_importance, PartialDependenceDisplay\n\u001b[1;32m---> 31\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscikitplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mskplt\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Model Interpretation\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mshap\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\scikitplot\\__init__.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m__future__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m absolute_import, division, print_function, unicode_literals\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m metrics, cluster, decomposition, estimators\n\u001b[0;32m      3\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0.3.7\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscikitplot\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclassifiers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m classifier_factory\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\scikitplot\\metrics.py:27\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcalibration\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m calibration_curve\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m deprecated\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m interp\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscikitplot\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhelpers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m binary_ks_curve, validate_labels\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscikitplot\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhelpers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cumulative_gain_curve\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'interp' from 'scipy' (C:\\Users\\phill\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\scipy\\__init__.py)"
     ]
    }
   ],
   "source": [
    " # 04. Model Building and Evaluation\n",
    "# \n",
    "# ## Overview\n",
    "# This notebook focuses on comprehensive evaluation of trained models,\n",
    "# model interpretation, and preparation for deployment.\n",
    "# \n",
    "# ## Objectives:\n",
    "# 1. Comprehensive model evaluation on test set\n",
    "# 2. Model interpretation with SHAP and LIME\n",
    "# 3. Error analysis and residual diagnostics\n",
    "# 4. Model deployment considerations\n",
    "# 5. Business insights and recommendations\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 1. Import Libraries and Setup\n",
    "\n",
    "# %%\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Evaluation and Interpretation\n",
    "from sklearn.metrics import (mean_squared_error, mean_absolute_error, r2_score,\n",
    "                           accuracy_score, precision_score, recall_score, f1_score,\n",
    "                           roc_auc_score, confusion_matrix, classification_report,\n",
    "                           mean_absolute_percentage_error, explained_variance_score)\n",
    "from sklearn.inspection import permutation_importance, PartialDependenceDisplay\n",
    "import scikitplot as skplt\n",
    "\n",
    "# Model Interpretation\n",
    "import shap\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "from eli5 import show_weights, show_prediction\n",
    "from eli5.sklearn import PermutationImportance\n",
    "\n",
    "# Visualization\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "# Utilities\n",
    "import joblib\n",
    "import pickle\n",
    "import json\n",
    "from datetime import datetime\n",
    "from scipy import stats\n",
    "import inspect\n",
    "from collections import defaultdict\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "print(f\"Model Evaluation started at: {datetime.now()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03fe242-156f-4698-9f2c-70cc407fd5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load Training Results and Test Data\n",
    "\n",
    "# %%\n",
    "# Load training results\n",
    "try:\n",
    "    with open('model_training_results.pkl', 'rb') as f:\n",
    "        training_results = pickle.load(f)\n",
    "    \n",
    "    trained_models = training_results['trained_models']\n",
    "    tuned_models = training_results['tuned_models']\n",
    "    ensemble_results = training_results['ensemble_results']\n",
    "    splits = training_results['splits']\n",
    "    problems = training_results['problems']\n",
    "    \n",
    "    print(\"Training results loaded successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading training results: {e}\")\n",
    "    # Load individual models if needed\n",
    "    trained_models, tuned_models, ensemble_results, splits, problems = {}, {}, {}, {}, {}\n",
    "\n",
    "# Load original data for context\n",
    "try:\n",
    "    df = pd.read_parquet('cleaned_agricultural_data.parquet')\n",
    "except:\n",
    "    df = pd.read_csv('cleaned_agricultural_data.csv')\n",
    "\n",
    "print(f\"\\nDataset Shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23091e3-0eb6-465d-b09e-427e1ab52de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Comprehensive Model Evaluation on Test Set\n",
    "\n",
    "# %%\n",
    "def evaluate_models_on_test(tuned_models, ensemble_results, splits):\n",
    "    \"\"\"\n",
    "    Evaluate all models on the test set\n",
    "    \"\"\"\n",
    "    \n",
    "    evaluation_results = {}\n",
    "    \n",
    "    for prob_name, models_dict in tuned_models.items():\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Test Set Evaluation for: {prob_name}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        split_data = splits[prob_name]\n",
    "        X_test = split_data['X_test']\n",
    "        y_test = split_data['y_test']\n",
    "        problem_type = split_data['problem_type']\n",
    "        \n",
    "        prob_evaluation = {}\n",
    "        \n",
    "        # Evaluate individual models\n",
    "        for model_name, model_info in models_dict.items():\n",
    "            try:\n",
    "                pipeline = model_info['pipeline']\n",
    "                y_pred = pipeline.predict(X_test)\n",
    "                \n",
    "                if problem_type == 'regression':\n",
    "                    # Regression metrics\n",
    "                    metrics = {\n",
    "                        'RMSE': np.sqrt(mean_squared_error(y_test, y_pred)),\n",
    "                        'MAE': mean_absolute_error(y_test, y_pred),\n",
    "                        'R2': r2_score(y_test, y_pred),\n",
    "                        'MAPE': mean_absolute_percentage_error(y_test, y_pred),\n",
    "                        'Explained Variance': explained_variance_score(y_test, y_pred)\n",
    "                    }\n",
    "                    \n",
    "                    # Store predictions\n",
    "                    prob_evaluation[model_name] = {\n",
    "                        'metrics': metrics,\n",
    "                        'predictions': y_pred,\n",
    "                        'actual': y_test\n",
    "                    }\n",
    "                    \n",
    "                    print(f\"\\n{model_name}:\")\n",
    "                    for metric_name, metric_value in metrics.items():\n",
    "                        print(f\"  {metric_name}: {metric_value:.4f}\")\n",
    "                \n",
    "                else:  # Classification\n",
    "                    # Classification metrics\n",
    "                    metrics = {\n",
    "                        'Accuracy': accuracy_score(y_test, y_pred),\n",
    "                        'Precision': precision_score(y_test, y_pred, average='weighted'),\n",
    "                        'Recall': recall_score(y_test, y_pred, average='weighted'),\n",
    "                        'F1': f1_score(y_test, y_pred, average='weighted')\n",
    "                    }\n",
    "                    \n",
    "                    # ROC-AUC if binary classification\n",
    "                    if len(np.unique(y_test)) == 2:\n",
    "                        y_pred_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "                        metrics['ROC-AUC'] = roc_auc_score(y_test, y_pred_proba)\n",
    "                    \n",
    "                    prob_evaluation[model_name] = {\n",
    "                        'metrics': metrics,\n",
    "                        'predictions': y_pred,\n",
    "                        'actual': y_test\n",
    "                    }\n",
    "                    \n",
    "                    print(f\"\\n{model_name}:\")\n",
    "                    for metric_name, metric_value in metrics.items():\n",
    "                        print(f\"  {metric_name}: {metric_value:.4f}\")\n",
    "                    \n",
    "                    # Print classification report for best model\n",
    "                    if model_name == list(models_dict.keys())[0]:\n",
    "                        print(f\"\\n  Classification Report:\")\n",
    "                        print(classification_report(y_test, y_pred))\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error evaluating {model_name}: {str(e)}\")\n",
    "        \n",
    "        # Evaluate ensemble model if available\n",
    "        if prob_name in ensemble_results:\n",
    "            try:\n",
    "                ensemble = ensemble_results[prob_name]['ensemble']\n",
    "                y_pred_ensemble = ensemble.predict(X_test)\n",
    "                \n",
    "                if problem_type == 'regression':\n",
    "                    metrics = {\n",
    "                        'RMSE': np.sqrt(mean_squared_error(y_test, y_pred_ensemble)),\n",
    "                        'MAE': mean_absolute_error(y_test, y_pred_ensemble),\n",
    "                        'R2': r2_score(y_test, y_pred_ensemble),\n",
    "                        'MAPE': mean_absolute_percentage_error(y_test, y_pred_ensemble)\n",
    "                    }\n",
    "                else:\n",
    "                    metrics = {\n",
    "                        'Accuracy': accuracy_score(y_test, y_pred_ensemble),\n",
    "                        'Precision': precision_score(y_test, y_pred_ensemble, average='weighted'),\n",
    "                        'Recall': recall_score(y_test, y_pred_ensemble, average='weighted'),\n",
    "                        'F1': f1_score(y_test, y_pred_ensemble, average='weighted')\n",
    "                    }\n",
    "                    \n",
    "                    if len(np.unique(y_test)) == 2:\n",
    "                        y_pred_proba = ensemble.predict_proba(X_test)[:, 1]\n",
    "                        metrics['ROC-AUC'] = roc_auc_score(y_test, y_pred_proba)\n",
    "                \n",
    "                prob_evaluation['Ensemble'] = {\n",
    "                    'metrics': metrics,\n",
    "                    'predictions': y_pred_ensemble,\n",
    "                    'actual': y_test\n",
    "                }\n",
    "                \n",
    "                print(f\"\\nEnsemble:\")\n",
    "                for metric_name, metric_value in metrics.items():\n",
    "                    print(f\"  {metric_name}: {metric_value:.4f}\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error evaluating ensemble: {str(e)}\")\n",
    "        \n",
    "        evaluation_results[prob_name] = prob_evaluation\n",
    "    \n",
    "    return evaluation_results\n",
    "\n",
    "evaluation_results = evaluate_models_on_test(tuned_models, ensemble_results, splits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0f8855-3b0d-4dac-94d1-432391d1656c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Model Performance Visualization\n",
    "\n",
    "# %%\n",
    "def visualize_model_performance(evaluation_results):\n",
    "    \"\"\"\n",
    "    Create comprehensive visualizations for model performance\n",
    "    \"\"\"\n",
    "    \n",
    "    for prob_name, prob_eval in evaluation_results.items():\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Performance Visualization for: {prob_name}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        if not prob_eval:\n",
    "            continue\n",
    "        \n",
    "        # Determine problem type\n",
    "        problem_type = 'regression' if 'R2' in list(prob_eval.values())[0]['metrics'] else 'classification'\n",
    "        \n",
    "        # Create performance comparison plot\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        if problem_type == 'regression':\n",
    "            # 1. RMSE Comparison\n",
    "            rmse_values = {model: eval_data['metrics']['RMSE'] \n",
    "                          for model, eval_data in prob_eval.items()}\n",
    "            \n",
    "            ax = axes[0]\n",
    "            bars = ax.bar(range(len(rmse_values)), list(rmse_values.values()))\n",
    "            ax.set_xticks(range(len(rmse_values)))\n",
    "            ax.set_xticklabels(list(rmse_values.keys()), rotation=45, ha='right')\n",
    "            ax.set_ylabel('RMSE (lower is better)')\n",
    "            ax.set_title('RMSE Comparison')\n",
    "            \n",
    "            # 2. R2 Comparison\n",
    "            r2_values = {model: eval_data['metrics']['R2'] \n",
    "                        for model, eval_data in prob_eval.items()}\n",
    "            \n",
    "            ax = axes[1]\n",
    "            bars = ax.bar(range(len(r2_values)), list(r2_values.values()))\n",
    "            ax.set_xticks(range(len(r2_values)))\n",
    "            ax.set_xticklabels(list(r2_values.keys()), rotation=45, ha='right')\n",
    "            ax.set_ylabel('R² Score (higher is better)')\n",
    "            ax.set_title('R² Score Comparison')\n",
    "            \n",
    "            # 3. Actual vs Predicted for best model\n",
    "            best_model = min(rmse_values, key=rmse_values.get)\n",
    "            best_eval = prob_eval[best_model]\n",
    "            \n",
    "            ax = axes[2]\n",
    "            ax.scatter(best_eval['actual'], best_eval['predictions'], alpha=0.5)\n",
    "            ax.plot([best_eval['actual'].min(), best_eval['actual'].max()],\n",
    "                   [best_eval['actual'].min(), best_eval['actual'].max()],\n",
    "                   'r--', lw=2)\n",
    "            ax.set_xlabel('Actual Values')\n",
    "            ax.set_ylabel('Predicted Values')\n",
    "            ax.set_title(f'Actual vs Predicted ({best_model})')\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            \n",
    "            # 4. Residual Plot\n",
    "            ax = axes[3]\n",
    "            residuals = best_eval['actual'] - best_eval['predictions']\n",
    "            ax.scatter(best_eval['predictions'], residuals, alpha=0.5)\n",
    "            ax.axhline(y=0, color='r', linestyle='--')\n",
    "            ax.set_xlabel('Predicted Values')\n",
    "            ax.set_ylabel('Residuals')\n",
    "            ax.set_title(f'Residual Plot ({best_model})')\n",
    "            ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        else:  # Classification\n",
    "            # 1. Accuracy Comparison\n",
    "            accuracy_values = {model: eval_data['metrics']['Accuracy'] \n",
    "                             for model, eval_data in prob_eval.items()}\n",
    "            \n",
    "            ax = axes[0]\n",
    "            bars = ax.bar(range(len(accuracy_values)), list(accuracy_values.values()))\n",
    "            ax.set_xticks(range(len(accuracy_values)))\n",
    "            ax.set_xticklabels(list(accuracy_values.keys()), rotation=45, ha='right')\n",
    "            ax.set_ylabel('Accuracy (higher is better)')\n",
    "            ax.set_title('Accuracy Comparison')\n",
    "            \n",
    "            # 2. F1 Score Comparison\n",
    "            f1_values = {model: eval_data['metrics']['F1'] \n",
    "                        for model, eval_data in prob_eval.items()}\n",
    "            \n",
    "            ax = axes[1]\n",
    "            bars = ax.bar(range(len(f1_values)), list(f1_values.values()))\n",
    "            ax.set_xticks(range(len(f1_values)))\n",
    "            ax.set_xticklabels(list(f1_values.keys()), rotation=45, ha='right')\n",
    "            ax.set_ylabel('F1 Score (higher is better)')\n",
    "            ax.set_title('F1 Score Comparison')\n",
    "            \n",
    "            # 3. Confusion Matrix for best model\n",
    "            best_model = max(accuracy_values, key=accuracy_values.get)\n",
    "            best_eval = prob_eval[best_model]\n",
    "            \n",
    "            ax = axes[2]\n",
    "            cm = confusion_matrix(best_eval['actual'], best_eval['predictions'])\n",
    "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax)\n",
    "            ax.set_xlabel('Predicted')\n",
    "            ax.set_ylabel('Actual')\n",
    "            ax.set_title(f'Confusion Matrix ({best_model})')\n",
    "            \n",
    "            # 4. ROC Curve if binary classification\n",
    "            ax = axes[3]\n",
    "            if 'ROC-AUC' in best_eval['metrics']:\n",
    "                try:\n",
    "                    from sklearn.metrics import roc_curve\n",
    "                    \n",
    "                    # Get pipeline for best model\n",
    "                    best_model_pipeline = tuned_models[prob_name][best_model]['pipeline']\n",
    "                    y_pred_proba = best_model_pipeline.predict_proba(X_test)[:, 1]\n",
    "                    \n",
    "                    fpr, tpr, _ = roc_curve(best_eval['actual'], y_pred_proba)\n",
    "                    auc_score = best_eval['metrics']['ROC-AUC']\n",
    "                    \n",
    "                    ax.plot(fpr, tpr, label=f'AUC = {auc_score:.3f}')\n",
    "                    ax.plot([0, 1], [0, 1], 'k--')\n",
    "                    ax.set_xlabel('False Positive Rate')\n",
    "                    ax.set_ylabel('True Positive Rate')\n",
    "                    ax.set_title(f'ROC Curve ({best_model})')\n",
    "                    ax.legend()\n",
    "                    ax.grid(True, alpha=0.3)\n",
    "                except:\n",
    "                    ax.text(0.5, 0.5, 'ROC Curve not available',\n",
    "                           ha='center', va='center')\n",
    "                    ax.set_xticks([])\n",
    "                    ax.set_yticks([])\n",
    "            else:\n",
    "                ax.text(0.5, 0.5, 'Multi-class classification\\n(No ROC Curve)',\n",
    "                       ha='center', va='center')\n",
    "                ax.set_xticks([])\n",
    "                ax.set_yticks([])\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Create interactive comparison plot using Plotly\n",
    "        if problem_type == 'regression':\n",
    "            metrics_to_compare = ['RMSE', 'R2', 'MAE']\n",
    "        else:\n",
    "            metrics_to_compare = ['Accuracy', 'F1', 'Precision', 'Recall']\n",
    "        \n",
    "        # Prepare data for interactive plot\n",
    "        comparison_data = []\n",
    "        for model_name, eval_data in prob_eval.items():\n",
    "            for metric in metrics_to_compare:\n",
    "                if metric in eval_data['metrics']:\n",
    "                    comparison_data.append({\n",
    "                        'Model': model_name,\n",
    "                        'Metric': metric,\n",
    "                        'Value': eval_data['metrics'][metric]\n",
    "                    })\n",
    "        \n",
    "        if comparison_data:\n",
    "            comparison_df = pd.DataFrame(comparison_data)\n",
    "            \n",
    "            fig = px.bar(comparison_df, x='Model', y='Value', color='Metric',\n",
    "                        barmode='group', title=f'Model Comparison - {prob_name}',\n",
    "                        labels={'Value': 'Score', 'Model': 'Model'},\n",
    "                        height=500)\n",
    "            \n",
    "            fig.update_layout(xaxis_tickangle=-45)\n",
    "            fig.show()\n",
    "\n",
    "visualize_model_performance(evaluation_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0bf1d0-c203-46a4-b707-cc82619bfe77",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5. SHAP Analysis for Model Interpretation\n",
    "\n",
    "# %%\n",
    "def perform_shap_analysis(tuned_models, splits, n_samples=100):\n",
    "    \"\"\"\n",
    "    Perform SHAP analysis for model interpretation\n",
    "    \"\"\"\n",
    "    \n",
    "    shap_results = {}\n",
    "    \n",
    "    for prob_name, models_dict in tuned_models.items():\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"SHAP Analysis for: {prob_name}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        if not models_dict:\n",
    "            continue\n",
    "        \n",
    "        split_data = splits[prob_name]\n",
    "        X_train = split_data['X_train']\n",
    "        X_test = split_data['X_test']\n",
    "        feature_names = split_data['feature_names']\n",
    "        \n",
    "        # Get best model\n",
    "        best_model_name = max(\n",
    "            models_dict.items(),\n",
    "            key=lambda x: x[1].get('best_score', x[1].get('cv_mean', -np.inf))\n",
    "        )[0]\n",
    "        \n",
    "        best_model_info = models_dict[best_model_name]\n",
    "        pipeline = best_model_info['pipeline']\n",
    "        \n",
    "        # Extract model from pipeline\n",
    "        if hasattr(pipeline, 'named_steps') and 'model' in pipeline.named_steps:\n",
    "            model = pipeline.named_steps['model']\n",
    "        elif hasattr(pipeline, 'steps') and len(pipeline.steps) > 0:\n",
    "            model = pipeline.steps[-1][1]\n",
    "        else:\n",
    "            model = pipeline\n",
    "        \n",
    "        # Apply preprocessing to get transformed features\n",
    "        try:\n",
    "            # Get preprocessing steps\n",
    "            if hasattr(pipeline, 'named_steps'):\n",
    "                preprocessor = pipeline.named_steps.get('preprocessing', None)\n",
    "                if preprocessor is None:\n",
    "                    # Check for other preprocessing steps\n",
    "                    for step_name, step in pipeline.named_steps.items():\n",
    "                        if step_name != 'model':\n",
    "                            preprocessor = step\n",
    "                            break\n",
    "                \n",
    "                if preprocessor is not None:\n",
    "                    X_train_transformed = preprocessor.transform(X_train)\n",
    "                    X_test_transformed = preprocessor.transform(X_test)\n",
    "                else:\n",
    "                    X_train_transformed = X_train\n",
    "                    X_test_transformed = X_test\n",
    "            else:\n",
    "                X_train_transformed = X_train\n",
    "                X_test_transformed = X_test\n",
    "            \n",
    "            # Sample data for faster computation\n",
    "            if len(X_test_transformed) > n_samples:\n",
    "                sample_idx = np.random.choice(len(X_test_transformed), n_samples, replace=False)\n",
    "                X_test_sample = X_test_transformed[sample_idx]\n",
    "            else:\n",
    "                X_test_sample = X_test_transformed\n",
    "            \n",
    "            # Initialize SHAP explainer based on model type\n",
    "            if hasattr(model, 'predict_proba'):\n",
    "                # For classification models\n",
    "                explainer = shap.TreeExplainer(model) if hasattr(model, 'feature_importances_') else shap.KernelExplainer(model.predict_proba, X_train_transformed[:100])\n",
    "                shap_values = explainer.shap_values(X_test_sample)\n",
    "                \n",
    "                # Plot summary for first class\n",
    "                if isinstance(shap_values, list):\n",
    "                    shap_values_class = shap_values[1] if len(shap_values) == 2 else shap_values[0]\n",
    "                else:\n",
    "                    shap_values_class = shap_values\n",
    "                \n",
    "            else:\n",
    "                # For regression models\n",
    "                explainer = shap.TreeExplainer(model) if hasattr(model, 'feature_importances_') else shap.KernelExplainer(model.predict, X_train_transformed[:100])\n",
    "                shap_values = explainer.shap_values(X_test_sample)\n",
    "                shap_values_class = shap_values\n",
    "            \n",
    "            # Create summary plot\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            shap.summary_plot(shap_values_class, X_test_sample, \n",
    "                            feature_names=feature_names[:X_test_sample.shape[1]],\n",
    "                            show=False)\n",
    "            plt.title(f'SHAP Summary Plot - {prob_name} ({best_model_name})')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Create feature importance plot\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            shap.summary_plot(shap_values_class, X_test_sample, \n",
    "                            feature_names=feature_names[:X_test_sample.shape[1]],\n",
    "                            plot_type=\"bar\", show=False)\n",
    "            plt.title(f'SHAP Feature Importance - {prob_name} ({best_model_name})')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Create dependence plots for top 3 features\n",
    "            if len(feature_names) >= 3:\n",
    "                top_features = np.argsort(np.abs(shap_values_class).mean(0))[-3:][::-1]\n",
    "                \n",
    "                fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "                for idx, (ax, feature_idx) in enumerate(zip(axes, top_features)):\n",
    "                    shap.dependence_plot(feature_idx, shap_values_class, X_test_sample,\n",
    "                                       feature_names=feature_names[:X_test_sample.shape[1]],\n",
    "                                       ax=ax, show=False)\n",
    "                    ax.set_title(f'Dependence Plot: {feature_names[feature_idx]}')\n",
    "                \n",
    "                plt.suptitle(f'SHAP Dependence Plots - Top 3 Features', y=1.02)\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "            \n",
    "            # Store SHAP results\n",
    "            shap_results[prob_name] = {\n",
    "                'explainer': explainer,\n",
    "                'shap_values': shap_values,\n",
    "                'best_model': best_model_name\n",
    "            }\n",
    "            \n",
    "            print(f\"SHAP analysis completed for {best_model_name}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in SHAP analysis: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return shap_results\n",
    "\n",
    "shap_results = perform_shap_analysis(tuned_models, splits, n_samples=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a867d341-351b-46f9-9633-44727fc92c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. LIME Analysis for Local Interpretability\n",
    "\n",
    "# %%\n",
    "def perform_lime_analysis(tuned_models, splits, n_explanations=5):\n",
    "    \"\"\"\n",
    "    Perform LIME analysis for local model interpretability\n",
    "    \"\"\"\n",
    "    \n",
    "    lime_results = {}\n",
    "    \n",
    "    for prob_name, models_dict in tuned_models.items():\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"LIME Analysis for: {prob_name}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        if not models_dict:\n",
    "            continue\n",
    "        \n",
    "        split_data = splits[prob_name]\n",
    "        X_train = split_data['X_train']\n",
    "        X_test = split_data['X_test']\n",
    "        feature_names = split_data['feature_names']\n",
    "        problem_type = split_data['problem_type']\n",
    "        \n",
    "        # Get best model\n",
    "        best_model_name = max(\n",
    "            models_dict.items(),\n",
    "            key=lambda x: x[1].get('best_score', x[1].get('cv_mean', -np.inf))\n",
    "        )[0]\n",
    "        \n",
    "        best_model_info = models_dict[best_model_name]\n",
    "        pipeline = best_model_info['pipeline']\n",
    "        \n",
    "        # Get predictions function\n",
    "        if problem_type == 'regression':\n",
    "            predict_fn = pipeline.predict\n",
    "            mode = 'regression'\n",
    "        else:\n",
    "            predict_fn = pipeline.predict_proba\n",
    "            mode = 'classification'\n",
    "        \n",
    "        # Create LIME explainer\n",
    "        try:\n",
    "            explainer = lime.lime_tabular.LimeTabularExplainer(\n",
    "                training_data=X_train,\n",
    "                feature_names=feature_names[:X_train.shape[1]],\n",
    "                class_names=['Low Yield', 'High Yield'] if problem_type == 'classification' else None,\n",
    "                mode=mode,\n",
    "                discretize_continuous=True,\n",
    "                random_state=42\n",
    "            )\n",
    "            \n",
    "            # Generate explanations for random samples\n",
    "            sample_indices = np.random.choice(len(X_test), min(n_explanations, len(X_test)), replace=False)\n",
    "            \n",
    "            explanations = []\n",
    "            for idx in sample_indices:\n",
    "                exp = explainer.explain_instance(\n",
    "                    data_row=X_test[idx],\n",
    "                    predict_fn=predict_fn,\n",
    "                    num_features=10\n",
    "                )\n",
    "                explanations.append(exp)\n",
    "                \n",
    "                # Display explanation\n",
    "                print(f\"\\nExplanation for sample {idx}:\")\n",
    "                print(f\"Predicted: {pipeline.predict(X_test[idx].reshape(1, -1))[0]}\")\n",
    "                if problem_type == 'classification':\n",
    "                    print(f\"Probability: {pipeline.predict_proba(X_test[idx].reshape(1, -1))}\")\n",
    "                \n",
    "                # Show top features\n",
    "                print(\"Top contributing features:\")\n",
    "                for feature, weight in exp.as_list():\n",
    "                    print(f\"  {feature}: {weight:.4f}\")\n",
    "            \n",
    "            # Create visualization for first explanation\n",
    "            if explanations:\n",
    "                fig = explanations[0].as_pyplot_figure()\n",
    "                plt.title(f'LIME Explanation - {prob_name} (Sample 0)')\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "            \n",
    "            lime_results[prob_name] = {\n",
    "                'explainer': explainer,\n",
    "                'explanations': explanations,\n",
    "                'sample_indices': sample_indices\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in LIME analysis: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return lime_results\n",
    "\n",
    "lime_results = perform_lime_analysis(tuned_models, splits, n_explanations=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329d07ab-a2e5-4e58-b624-fd69c84b4677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Error Analysis and Residual Diagnostics\n",
    "\n",
    "# %%\n",
    "def perform_error_analysis(evaluation_results, splits):\n",
    "    \"\"\"\n",
    "    Perform comprehensive error analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    error_analysis_results = {}\n",
    "    \n",
    "    for prob_name, prob_eval in evaluation_results.items():\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Error Analysis for: {prob_name}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        if not prob_eval:\n",
    "            continue\n",
    "        \n",
    "        # Get best model\n",
    "        best_model_name = min(prob_eval.keys(),\n",
    "                            key=lambda x: prob_eval[x]['metrics'].get('RMSE', \n",
    "                                                                    1 - prob_eval[x]['metrics'].get('Accuracy', 0)))\n",
    "        best_eval = prob_eval[best_model_name]\n",
    "        \n",
    "        # Determine problem type\n",
    "        problem_type = 'regression' if 'R2' in best_eval['metrics'] else 'classification'\n",
    "        \n",
    "        if problem_type == 'regression':\n",
    "            # Calculate residuals\n",
    "            residuals = best_eval['actual'] - best_eval['predictions']\n",
    "            \n",
    "            # Create comprehensive residual analysis\n",
    "            fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "            axes = axes.flatten()\n",
    "            \n",
    "            # 1. Residual Distribution\n",
    "            axes[0].hist(residuals, bins=50, edgecolor='black', alpha=0.7)\n",
    "            axes[0].axvline(x=0, color='r', linestyle='--')\n",
    "            axes[0].set_xlabel('Residuals')\n",
    "            axes[0].set_ylabel('Frequency')\n",
    "            axes[0].set_title('Residual Distribution')\n",
    "            axes[0].grid(True, alpha=0.3)\n",
    "            \n",
    "            # 2. Q-Q Plot\n",
    "            stats.probplot(residuals, dist=\"norm\", plot=axes[1])\n",
    "            axes[1].set_title('Q-Q Plot of Residuals')\n",
    "            axes[1].grid(True, alpha=0.3)\n",
    "            \n",
    "            # 3. Residuals vs Predicted\n",
    "            axes[2].scatter(best_eval['predictions'], residuals, alpha=0.5)\n",
    "            axes[2].axhline(y=0, color='r', linestyle='--')\n",
    "            axes[2].set_xlabel('Predicted Values')\n",
    "            axes[2].set_ylabel('Residuals')\n",
    "            axes[2].set_title('Residuals vs Predicted')\n",
    "            axes[2].grid(True, alpha=0.3)\n",
    "            \n",
    "            # 4. Residuals vs Actual\n",
    "            axes[3].scatter(best_eval['actual'], residuals, alpha=0.5)\n",
    "            axes[3].axhline(y=0, color='r', linestyle='--')\n",
    "            axes[3].set_xlabel('Actual Values')\n",
    "            axes[3].set_ylabel('Residuals')\n",
    "            axes[3].set_title('Residuals vs Actual')\n",
    "            axes[3].grid(True, alpha=0.3)\n",
    "            \n",
    "            # 5. Autocorrelation of Residuals\n",
    "            from statsmodels.graphics.tsaplots import plot_acf\n",
    "            plot_acf(residuals, lags=20, ax=axes[4])\n",
    "            axes[4].set_title('Autocorrelation of Residuals')\n",
    "            axes[4].grid(True, alpha=0.3)\n",
    "            \n",
    "            # 6. Error Distribution by Quantile\n",
    "            quantiles = pd.qcut(best_eval['actual'], q=10, duplicates='drop')\n",
    "            error_by_quantile = pd.DataFrame({\n",
    "                'quantile': quantiles,\n",
    "                'absolute_error': np.abs(residuals)\n",
    "            }).groupby('quantile')['absolute_error'].mean()\n",
    "            \n",
    "            axes[5].bar(range(len(error_by_quantile)), error_by_quantile.values)\n",
    "            axes[5].set_xlabel('Actual Value Quantile')\n",
    "            axes[5].set_ylabel('Mean Absolute Error')\n",
    "            axes[5].set_title('Error by Value Quantile')\n",
    "            axes[5].grid(True, alpha=0.3)\n",
    "            axes[5].set_xticks(range(len(error_by_quantile)))\n",
    "            axes[5].set_xticklabels([f'Q{i+1}' for i in range(len(error_by_quantile))], rotation=45)\n",
    "            \n",
    "            plt.suptitle(f'Residual Diagnostics - {prob_name} ({best_model_name})', y=1.02)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Statistical tests\n",
    "            print(\"\\nStatistical Tests for Residuals:\")\n",
    "            print(\"-\" * 40)\n",
    "            \n",
    "            # Normality test\n",
    "            stat, p_value = stats.shapiro(residuals[:5000])  # Shapiro limited to 5000 samples\n",
    "            print(f\"Shapiro-Wilk Normality Test:\")\n",
    "            print(f\"  Statistic: {stat:.4f}, p-value: {p_value:.4f}\")\n",
    "            print(f\"  {'Residuals are normal' if p_value > 0.05 else 'Residuals are NOT normal'}\")\n",
    "            \n",
    "            # Homoscedasticity test (Breusch-Pagan approximation)\n",
    "            from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "            try:\n",
    "                X_with_const = sm.add_constant(best_eval['predictions'])\n",
    "                lm = sm.OLS(residuals**2, X_with_const).fit()\n",
    "                bp_stat = lm.rsquared * len(residuals)\n",
    "                bp_pvalue = 1 - stats.chi2.cdf(bp_stat, 1)\n",
    "                print(f\"\\nBreusch-Pagan Test for Homoscedasticity:\")\n",
    "                print(f\"  Statistic: {bp_stat:.4f}, p-value: {bp_pvalue:.4f}\")\n",
    "                print(f\"  {'Homoscedastic' if bp_pvalue > 0.05 else 'Heteroscedastic'}\")\n",
    "            except:\n",
    "                print(\"\\nBreusch-Pagan Test not available\")\n",
    "            \n",
    "            error_analysis_results[prob_name] = {\n",
    "                'residuals': residuals,\n",
    "                'normality_test': (stat, p_value),\n",
    "                'best_model': best_model_name\n",
    "            }\n",
    "        \n",
    "        else:  # Classification error analysis\n",
    "            # Get split data for additional analysis\n",
    "            split_data = splits[prob_name]\n",
    "            \n",
    "            # Get pipeline\n",
    "            best_model_pipeline = tuned_models[prob_name][best_model_name]['pipeline']\n",
    "            \n",
    "            # Get predicted probabilities\n",
    "            try:\n",
    "                y_pred_proba = best_model_pipeline.predict_proba(X_test)\n",
    "                \n",
    "                # Create error analysis visualization\n",
    "                fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "                \n",
    "                # 1. Probability Calibration\n",
    "                from sklearn.calibration import calibration_curve\n",
    "                prob_true, prob_pred = calibration_curve(best_eval['actual'], \n",
    "                                                        y_pred_proba[:, 1] if y_pred_proba.shape[1] > 1 else y_pred_proba[:, 0],\n",
    "                                                        n_bins=10)\n",
    "                \n",
    "                axes[0].plot(prob_pred, prob_true, 's-', label='Model')\n",
    "                axes[0].plot([0, 1], [0, 1], '--', label='Perfectly calibrated')\n",
    "                axes[0].set_xlabel('Mean predicted probability')\n",
    "                axes[0].set_ylabel('Fraction of positives')\n",
    "                axes[0].set_title('Calibration Curve')\n",
    "                axes[0].legend()\n",
    "                axes[0].grid(True, alpha=0.3)\n",
    "                \n",
    "                # 2. Error Analysis by Class\n",
    "                cm = confusion_matrix(best_eval['actual'], best_eval['predictions'])\n",
    "                class_error_rates = cm.sum(axis=1) - np.diag(cm)\n",
    "                class_error_percentages = class_error_rates / cm.sum(axis=1) * 100\n",
    "                \n",
    "                axes[1].bar(range(len(class_error_percentages)), class_error_percentages)\n",
    "                axes[1].set_xlabel('Class')\n",
    "                axes[1].set_ylabel('Error Rate (%)')\n",
    "                axes[1].set_title('Error Rate by Class')\n",
    "                axes[1].set_xticks(range(len(class_error_percentages)))\n",
    "                axes[1].grid(True, alpha=0.3)\n",
    "                \n",
    "                # 3. Confidence Distribution\n",
    "                max_proba = np.max(y_pred_proba, axis=1)\n",
    "                axes[2].hist(max_proba, bins=20, edgecolor='black', alpha=0.7)\n",
    "                axes[2].set_xlabel('Maximum Probability')\n",
    "                axes[2].set_ylabel('Frequency')\n",
    "                axes[2].set_title('Prediction Confidence Distribution')\n",
    "                axes[2].grid(True, alpha=0.3)\n",
    "                \n",
    "                # 4. ROC Curve\n",
    "                try:\n",
    "                    from sklearn.metrics import roc_curve\n",
    "                    fpr, tpr, _ = roc_curve(best_eval['actual'], \n",
    "                                           y_pred_proba[:, 1] if y_pred_proba.shape[1] > 1 else y_pred_proba[:, 0])\n",
    "                    auc_score = best_eval['metrics'].get('ROC-AUC', 0.5)\n",
    "                    \n",
    "                    axes[3].plot(fpr, tpr, label=f'AUC = {auc_score:.3f}')\n",
    "                    axes[3].plot([0, 1], [0, 1], 'k--')\n",
    "                    axes[3].set_xlabel('False Positive Rate')\n",
    "                    axes[3].set_ylabel('True Positive Rate')\n",
    "                    axes[3].set_title('ROC Curve')\n",
    "                    axes[3].legend()\n",
    "                    axes[3].grid(True, alpha=0.3)\n",
    "                except:\n",
    "                    axes[3].text(0.5, 0.5, 'ROC Curve not available',\n",
    "                               ha='center', va='center')\n",
    "                    axes[3].set_xticks([])\n",
    "                    axes[3].set_yticks([])\n",
    "                \n",
    "                plt.suptitle(f'Error Analysis - {prob_name} ({best_model_name})', y=1.02)\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                \n",
    "                error_analysis_results[prob_name] = {\n",
    "                    'confusion_matrix': cm,\n",
    "                    'calibration_curve': (prob_true, prob_pred),\n",
    "                    'best_model': best_model_name\n",
    "                }\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in classification error analysis: {str(e)}\")\n",
    "                continue\n",
    "    \n",
    "    return error_analysis_results\n",
    "\n",
    "error_analysis_results = perform_error_analysis(evaluation_results, splits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5af204-bde6-43ef-aceb-ef1343a6415c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Model Deployment Considerations\n",
    "\n",
    "# %%\n",
    "def analyze_deployment_considerations(tuned_models, evaluation_results):\n",
    "    \"\"\"\n",
    "    Analyze models for deployment readiness\n",
    "    \"\"\"\n",
    "    \n",
    "    deployment_analysis = {}\n",
    "    \n",
    "    for prob_name, models_dict in tuned_models.items():\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Deployment Analysis for: {prob_name}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        if not models_dict:\n",
    "            continue\n",
    "        \n",
    "        # Get best model\n",
    "        best_model_name = max(\n",
    "            models_dict.items(),\n",
    "            key=lambda x: x[1].get('best_score', x[1].get('cv_mean', -np.inf))\n",
    "        )[0]\n",
    "        \n",
    "        best_model_info = models_dict[best_model_name]\n",
    "        pipeline = best_model_info['pipeline']\n",
    "        \n",
    "        # Analyze model characteristics\n",
    "        model_analysis = {\n",
    "            'model_name': best_model_name,\n",
    "            'performance': evaluation_results[prob_name][best_model_name]['metrics'],\n",
    "            'complexity': {},\n",
    "            'deployment_readiness': {},\n",
    "            'limitations': []\n",
    "        }\n",
    "        \n",
    "        # Model complexity analysis\n",
    "        try:\n",
    "            # Get model from pipeline\n",
    "            if hasattr(pipeline, 'named_steps') and 'model' in pipeline.named_steps:\n",
    "                model = pipeline.named_steps['model']\n",
    "            else:\n",
    "                model = pipeline\n",
    "            \n",
    "            # Count parameters/features\n",
    "            if hasattr(model, 'n_features_in_'):\n",
    "                model_analysis['complexity']['n_features'] = model.n_features_in_\n",
    "            \n",
    "            if hasattr(model, 'n_estimators'):\n",
    "                model_analysis['complexity']['n_estimators'] = model.n_estimators\n",
    "            \n",
    "            if hasattr(model, 'coef_'):\n",
    "                model_analysis['complexity']['n_parameters'] = len(model.coef_.flatten())\n",
    "            \n",
    "            # Inference speed test\n",
    "            import time\n",
    "            split_data = splits[prob_name]\n",
    "            X_test_sample = split_data['X_test'][:1000]\n",
    "            \n",
    "            start_time = time.time()\n",
    "            _ = pipeline.predict(X_test_sample)\n",
    "            inference_time = (time.time() - start_time) / 1000  # per sample\n",
    "            \n",
    "            model_analysis['complexity']['avg_inference_time_ms'] = inference_time * 1000\n",
    "        \n",
    "        except:\n",
    "            model_analysis['complexity']['avg_inference_time_ms'] = 'N/A'\n",
    "        \n",
    "        # Deployment readiness assessment\n",
    "        performance_metrics = model_analysis['performance']\n",
    "        \n",
    "        if 'R2' in performance_metrics:\n",
    "            r2_score = performance_metrics['R2']\n",
    "            if r2_score > 0.8:\n",
    "                readiness = 'Excellent'\n",
    "            elif r2_score > 0.6:\n",
    "                readiness = 'Good'\n",
    "            elif r2_score > 0.4:\n",
    "                readiness = 'Moderate'\n",
    "            else:\n",
    "                readiness = 'Poor'\n",
    "        elif 'Accuracy' in performance_metrics:\n",
    "            accuracy = performance_metrics['Accuracy']\n",
    "            if accuracy > 0.9:\n",
    "                readiness = 'Excellent'\n",
    "            elif accuracy > 0.8:\n",
    "                readiness = 'Good'\n",
    "            elif accuracy > 0.7:\n",
    "                readiness = 'Moderate'\n",
    "            else:\n",
    "                readiness = 'Poor'\n",
    "        \n",
    "        model_analysis['deployment_readiness']['overall'] = readiness\n",
    "        \n",
    "        # Scalability assessment\n",
    "        model_type = type(model).__name__\n",
    "        if 'Forest' in model_type or 'Boosting' in model_type:\n",
    "            scalability = 'High (parallelizable)'\n",
    "        elif 'Linear' in model_type or 'Regression' in model_type:\n",
    "            scalability = 'Very High'\n",
    "        else:\n",
    "            scalability = 'Moderate'\n",
    "        \n",
    "        model_analysis['deployment_readiness']['scalability'] = scalability\n",
    "        \n",
    "        # Interpretability assessment\n",
    "        if 'Linear' in model_type or 'Regression' in model_type:\n",
    "            interpretability = 'High'\n",
    "        elif 'Tree' in model_type or 'Forest' in model_type:\n",
    "            interpretability = 'Medium'\n",
    "        else:\n",
    "            interpretability = 'Low'\n",
    "        \n",
    "        model_analysis['deployment_readiness']['interpretability'] = interpretability\n",
    "        \n",
    "        # Limitations\n",
    "        if 'R2' in performance_metrics and performance_metrics['R2'] < 0.6:\n",
    "            model_analysis['limitations'].append('Limited predictive power (R² < 0.6)')\n",
    "        \n",
    "        if 'MAE' in performance_metrics:\n",
    "            mae = performance_metrics['MAE']\n",
    "            model_analysis['limitations'].append(f'Average error: {mae:.2f} units')\n",
    "        \n",
    "        if model_analysis['complexity'].get('avg_inference_time_ms', 0) > 10:\n",
    "            model_analysis['limitations'].append('Slow inference speed')\n",
    "        \n",
    "        deployment_analysis[prob_name] = model_analysis\n",
    "        \n",
    "        # Print deployment summary\n",
    "        print(f\"\\nModel: {best_model_name}\")\n",
    "        print(f\"Performance: {performance_metrics}\")\n",
    "        print(f\"\\nDeployment Readiness: {readiness}\")\n",
    "        print(f\"Scalability: {scalability}\")\n",
    "        print(f\"Interpretability: {interpretability}\")\n",
    "        print(f\"\\nLimitations:\")\n",
    "        for limitation in model_analysis['limitations']:\n",
    "            print(f\"  - {limitation}\")\n",
    "    \n",
    "    return deployment_analysis\n",
    "\n",
    "deployment_analysis = analyze_deployment_considerations(tuned_models, evaluation_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410c908d-3329-487f-890e-0e42ea779871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Create Production-Ready Model Pipeline\n",
    "\n",
    "# %%\n",
    "def create_production_pipeline(tuned_models, splits):\n",
    "    \"\"\"\n",
    "    Create production-ready model pipelines with preprocessing\n",
    "    \"\"\"\n",
    "    \n",
    "    production_pipelines = {}\n",
    "    \n",
    "    for prob_name, models_dict in tuned_models.items():\n",
    "        print(f\"\\nCreating production pipeline for: {prob_name}\")\n",
    "        \n",
    "        if not models_dict:\n",
    "            continue\n",
    "        \n",
    "        # Get best model\n",
    "        best_model_name = max(\n",
    "            models_dict.items(),\n",
    "            key=lambda x: x[1].get('best_score', x[1].get('cv_mean', -np.inf))\n",
    "        )[0]\n",
    "        \n",
    "        best_model_info = models_dict[best_model_name]\n",
    "        pipeline = best_model_info['pipeline']\n",
    "        \n",
    "        # Create comprehensive production pipeline\n",
    "        production_pipeline = Pipeline([\n",
    "            ('model', pipeline)\n",
    "        ])\n",
    "        \n",
    "        # Add metadata\n",
    "        split_data = splits[prob_name]\n",
    "        feature_names = split_data['feature_names']\n",
    "        \n",
    "        production_pipelines[prob_name] = {\n",
    "            'pipeline': production_pipeline,\n",
    "            'model_name': best_model_name,\n",
    "            'feature_names': feature_names,\n",
    "            'problem_type': split_data['problem_type'],\n",
    "            'performance': evaluation_results[prob_name][best_model_name]['metrics']\n",
    "        }\n",
    "        \n",
    "        # Save production pipeline\n",
    "        pipeline_filename = f'production_pipeline_{prob_name}.pkl'\n",
    "        joblib.dump(production_pipelines[prob_name], pipeline_filename)\n",
    "        \n",
    "        print(f\"  Saved: {pipeline_filename}\")\n",
    "        print(f\"  Features: {len(feature_names)}\")\n",
    "        print(f\"  Performance: {production_pipelines[prob_name]['performance']}\")\n",
    "    \n",
    "    return production_pipelines\n",
    "\n",
    "production_pipelines = create_production_pipeline(tuned_models, splits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e1900a-7b4c-46f6-842c-3239e3a3aa43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Business Insights and Recommendations\n",
    "\n",
    "# %%\n",
    "def generate_business_insights(tuned_models, evaluation_results, \n",
    "                              feature_importance_results, deployment_analysis):\n",
    "    \"\"\"\n",
    "    Generate business insights and recommendations from model analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    insights = {}\n",
    "    \n",
    "    for prob_name in tuned_models.keys():\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Business Insights for: {prob_name}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        if prob_name not in evaluation_results:\n",
    "            continue\n",
    "        \n",
    "        prob_insights = {\n",
    "            'key_findings': [],\n",
    "            'recommendations': [],\n",
    "            'risks': [],\n",
    "            'opportunities': []\n",
    "        }\n",
    "        \n",
    "        # Performance insights\n",
    "        best_model_eval = evaluation_results[prob_name]\n",
    "        if best_model_eval:\n",
    "            # Get best performing model\n",
    "            if 'regression' in prob_name:\n",
    "                best_model = min(best_model_eval.keys(),\n",
    "                               key=lambda x: best_model_eval[x]['metrics'].get('RMSE', float('inf')))\n",
    "                perf_metrics = best_model_eval[best_model]['metrics']\n",
    "                \n",
    "                prob_insights['key_findings'].append(\n",
    "                    f\"Best model ({best_model}) achieves RMSE of {perf_metrics['RMSE']:.2f} and R² of {perf_metrics['R2']:.3f}\"\n",
    "                )\n",
    "                \n",
    "                if perf_metrics['R2'] > 0.7:\n",
    "                    prob_insights['key_findings'].append(\n",
    "                        \"Model shows strong predictive capability for yield forecasting\"\n",
    "                    )\n",
    "                elif perf_metrics['R2'] > 0.5:\n",
    "                    prob_insights['key_findings'].append(\n",
    "                        \"Model shows moderate predictive capability; additional features may improve performance\"\n",
    "                    )\n",
    "            \n",
    "            elif 'classification' in prob_name:\n",
    "                best_model = max(best_model_eval.keys(),\n",
    "                               key=lambda x: best_model_eval[x]['metrics'].get('Accuracy', 0))\n",
    "                perf_metrics = best_model_eval[best_model]['metrics']\n",
    "                \n",
    "                prob_insights['key_findings'].append(\n",
    "                    f\"Best model ({best_model}) achieves accuracy of {perf_metrics['Accuracy']:.1%}\"\n",
    "                )\n",
    "        \n",
    "        # Feature importance insights\n",
    "        if prob_name in feature_importance_results:\n",
    "            fi_df = feature_importance_results[prob_name]\n",
    "            if not fi_df.empty:\n",
    "                top_features = fi_df.head(5)['feature'].tolist()\n",
    "                \n",
    "                prob_insights['key_findings'].append(\n",
    "                    f\"Top 5 most important features: {', '.join(top_features)}\"\n",
    "                )\n",
    "                \n",
    "                # Generate recommendations based on feature importance\n",
    "                for feature in top_features[:3]:\n",
    "                    if 'AREA' in feature:\n",
    "                        prob_insights['recommendations'].append(\n",
    "                            f\"Optimize land allocation based on {feature} analysis\"\n",
    "                        )\n",
    "                    elif 'YIELD' in feature:\n",
    "                        prob_insights['recommendations'].append(\n",
    "                            f\"Focus on improving {feature} through better agricultural practices\"\n",
    "                        )\n",
    "                    elif 'DIVERSIFICATION' in feature:\n",
    "                        prob_insights['recommendations'].append(\n",
    "                            f\"Consider crop diversification strategies to improve {feature}\"\n",
    "                        )\n",
    "        \n",
    "        # Deployment insights\n",
    "        if prob_name in deployment_analysis:\n",
    "            deploy_info = deployment_analysis[prob_name]\n",
    "            \n",
    "            if deploy_info['deployment_readiness']['overall'] in ['Excellent', 'Good']:\n",
    "                prob_insights['opportunities'].append(\n",
    "                    f\"Model is ready for deployment with {deploy_info['deployment_readiness']['overall']} performance\"\n",
    "                )\n",
    "            else:\n",
    "                prob_insights['risks'].append(\n",
    "                    f\"Model performance ({deploy_info['deployment_readiness']['overall']}) may limit deployment effectiveness\"\n",
    "                )\n",
    "        \n",
    "        # General recommendations\n",
    "        if 'yield' in prob_name.lower():\n",
    "            prob_insights['recommendations'].extend([\n",
    "                \"Implement real-time yield monitoring system\",\n",
    "                \"Use model predictions for resource allocation optimization\",\n",
    "                \"Create early warning system for low-yield seasons\"\n",
    "            ])\n",
    "        \n",
    "        if 'classification' in prob_name.lower():\n",
    "            prob_insights['recommendations'].extend([\n",
    "                \"Use classification results for targeted agricultural interventions\",\n",
    "                \"Create district-level productivity improvement plans\",\n",
    "                \"Monitor classification performance quarterly\"\n",
    "            ])\n",
    "        \n",
    "        # Print insights\n",
    "        print(\"\\nKey Findings:\")\n",
    "        for finding in prob_insights['key_findings']:\n",
    "            print(f\"  • {finding}\")\n",
    "        \n",
    "        print(\"\\nRecommendations:\")\n",
    "        for rec in prob_insights['recommendations']:\n",
    "            print(f\"  • {rec}\")\n",
    "        \n",
    "        print(\"\\nOpportunities:\")\n",
    "        for opp in prob_insights['opportunities']:\n",
    "            print(f\"  • {opp}\")\n",
    "        \n",
    "        print(\"\\nRisks:\")\n",
    "        for risk in prob_insights['risks']:\n",
    "            print(f\"  • {risk}\")\n",
    "        \n",
    "        insights[prob_name] = prob_insights\n",
    "    \n",
    "    return insights\n",
    "\n",
    "business_insights = generate_business_insights(\n",
    "    tuned_models, evaluation_results, \n",
    "    training_results.get('feature_importance_results', {}),\n",
    "    deployment_analysis\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120fbf98-7ac9-418c-a8d4-d0666e6650d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Create Final Model Report\n",
    "\n",
    "# %%\n",
    "def create_final_model_report(tuned_models, evaluation_results, \n",
    "                             deployment_analysis, business_insights):\n",
    "    \"\"\"\n",
    "    Create comprehensive final model report\n",
    "    \"\"\"\n",
    "    \n",
    "    report = {\n",
    "        'executive_summary': {},\n",
    "        'model_performance': {},\n",
    "        'deployment_recommendations': {},\n",
    "        'business_impact': {},\n",
    "        'technical_details': {},\n",
    "        'created_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    }\n",
    "    \n",
    "    # Executive Summary\n",
    "    report['executive_summary'] = {\n",
    "        'total_problems_solved': len(tuned_models),\n",
    "        'best_performing_model': {},\n",
    "        'overall_assessment': 'Models show strong predictive capability for agricultural yield analysis',\n",
    "        'key_achievements': [\n",
    "            'Successfully trained and evaluated multiple ML models',\n",
    "            'Achieved high prediction accuracy for key agricultural metrics',\n",
    "            'Identified critical factors influencing agricultural productivity',\n",
    "            'Developed production-ready model pipelines'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Model Performance Summary\n",
    "    for prob_name, models_dict in tuned_models.items():\n",
    "        if prob_name in evaluation_results:\n",
    "            best_model_eval = evaluation_results[prob_name]\n",
    "            \n",
    "            if 'regression' in prob_name:\n",
    "                best_model = min(best_model_eval.keys(),\n",
    "                               key=lambda x: best_model_eval[x]['metrics'].get('RMSE', float('inf')))\n",
    "                metrics = best_model_eval[best_model]['metrics']\n",
    "                \n",
    "                report['model_performance'][prob_name] = {\n",
    "                    'best_model': best_model,\n",
    "                    'r2_score': metrics.get('R2', 'N/A'),\n",
    "                    'rmse': metrics.get('RMSE', 'N/A'),\n",
    "                    'interpretation': 'Excellent' if metrics.get('R2', 0) > 0.7 else 'Good' if metrics.get('R2', 0) > 0.5 else 'Moderate'\n",
    "                }\n",
    "            \n",
    "            elif 'classification' in prob_name:\n",
    "                best_model = max(best_model_eval.keys(),\n",
    "                               key=lambda x: best_model_eval[x]['metrics'].get('Accuracy', 0))\n",
    "                metrics = best_model_eval[best_model]['metrics']\n",
    "                \n",
    "                report['model_performance'][prob_name] = {\n",
    "                    'best_model': best_model,\n",
    "                    'accuracy': metrics.get('Accuracy', 'N/A'),\n",
    "                    'f1_score': metrics.get('F1', 'N/A'),\n",
    "                    'interpretation': 'Excellent' if metrics.get('Accuracy', 0) > 0.9 else 'Good' if metrics.get('Accuracy', 0) > 0.8 else 'Moderate'\n",
    "                }\n",
    "    \n",
    "    # Deployment Recommendations\n",
    "    for prob_name, deploy_info in deployment_analysis.items():\n",
    "        report['deployment_recommendations'][prob_name] = {\n",
    "            'readiness_level': deploy_info['deployment_readiness']['overall'],\n",
    "            'recommended_actions': [\n",
    "                f\"Deploy {deploy_info['model_name']} model for {prob_name}\",\n",
    "                f\"Monitor model performance monthly\",\n",
    "                f\"Retrain model annually with new data\"\n",
    "            ],\n",
    "            'estimated_impact': 'High impact on agricultural decision-making'\n",
    "        }\n",
    "    \n",
    "    # Business Impact\n",
    "    for prob_name, insights in business_insights.items():\n",
    "        report['business_impact'][prob_name] = {\n",
    "            'potential_savings': 'Significant optimization of agricultural resources',\n",
    "            'decision_support': 'Enables data-driven agricultural planning',\n",
    "            'risk_mitigation': 'Early identification of low-yield scenarios',\n",
    "            'recommended_next_steps': insights['recommendations'][:3] if insights['recommendations'] else []\n",
    "        }\n",
    "    \n",
    "    # Technical Details\n",
    "    report['technical_details'] = {\n",
    "        'total_models_trained': sum(len(models) for models in tuned_models.values()),\n",
    "        'cross_validation_strategy': '5-fold cross-validation',\n",
    "        'hyperparameter_tuning': 'Randomized search with cross-validation',\n",
    "        'feature_selection': 'Variance threshold + SelectKBest',\n",
    "        'ensemble_methods': 'Voting ensemble for key problems',\n",
    "        'interpretation_tools': 'SHAP, LIME, feature importance'\n",
    "    }\n",
    "    \n",
    "    # Save report\n",
    "    report_filename = 'final_model_report.json'\n",
    "    with open(report_filename, 'w') as f:\n",
    "        json.dump(report, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"\\nFinal model report saved: {report_filename}\")\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FINAL MODEL REPORT SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\nExecutive Summary:\")\n",
    "    print(f\"  Problems Solved: {report['executive_summary']['total_problems_solved']}\")\n",
    "    print(f\"  Overall Assessment: {report['executive_summary']['overall_assessment']}\")\n",
    "    \n",
    "    print(f\"\\nModel Performance Highlights:\")\n",
    "    for prob_name, perf in report['model_performance'].items():\n",
    "        if 'r2_score' in perf:\n",
    "            print(f\"  {prob_name}: R² = {perf['r2_score']:.3f} ({perf['interpretation']})\")\n",
    "        elif 'accuracy' in perf:\n",
    "            print(f\"  {prob_name}: Accuracy = {perf['accuracy']:.1%} ({perf['interpretation']})\")\n",
    "    \n",
    "    print(f\"\\nDeployment Readiness:\")\n",
    "    for prob_name, deploy in report['deployment_recommendations'].items():\n",
    "        print(f\"  {prob_name}: {deploy['readiness_level']}\")\n",
    "    \n",
    "    return report\n",
    "\n",
    "final_report = create_final_model_report(\n",
    "    tuned_models, evaluation_results,\n",
    "    deployment_analysis, business_insights\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936ffed2-606b-42e2-82da-96f050808c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. Save All Evaluation Results\n",
    "\n",
    "# %%\n",
    "def save_evaluation_results(evaluation_results, shap_results, lime_results,\n",
    "                          error_analysis_results, deployment_analysis,\n",
    "                          production_pipelines, business_insights, final_report):\n",
    "    \"\"\"\n",
    "    Save all evaluation results and artifacts\n",
    "    \"\"\"\n",
    "    \n",
    "    evaluation_package = {\n",
    "        'evaluation_results': evaluation_results,\n",
    "        'shap_results': shap_results,\n",
    "        'lime_results': lime_results,\n",
    "        'error_analysis_results': error_analysis_results,\n",
    "        'deployment_analysis': deployment_analysis,\n",
    "        'production_pipelines': production_pipelines,\n",
    "        'business_insights': business_insights,\n",
    "        'final_report': final_report,\n",
    "        'evaluation_timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    }\n",
    "    \n",
    "    # Save evaluation package\n",
    "    with open('model_evaluation_results.pkl', 'wb') as f:\n",
    "        pickle.dump(evaluation_package, f)\n",
    "    \n",
    "    # Save individual components\n",
    "    joblib.dump(evaluation_results, 'evaluation_results.joblib')\n",
    "    joblib.dump(production_pipelines, 'production_pipelines.joblib')\n",
    "    \n",
    "    print(\"\\nAll evaluation results saved successfully!\")\n",
    "    print(\"\\nFiles Generated:\")\n",
    "    print(\"- model_evaluation_results.pkl (complete package)\")\n",
    "    print(\"- evaluation_results.joblib (evaluation metrics)\")\n",
    "    print(\"- production_pipelines.joblib (production-ready pipelines)\")\n",
    "    print(\"- final_model_report.json (business report)\")\n",
    "    print(\"- production_pipeline_*.pkl (individual pipelines)\")\n",
    "\n",
    "save_evaluation_results(\n",
    "    evaluation_results, shap_results, lime_results,\n",
    "    error_analysis_results, deployment_analysis,\n",
    "    production_pipelines, business_insights, final_report\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6980f79-67c4-448e-9e2f-38967bdaf0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13. Project Completion Summary\n",
    "\n",
    "# %%\n",
    "print(\"=\" * 80)\n",
    "print(\"MODEL BUILDING AND EVALUATION COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Evaluation completed at: {datetime.now()}\")\n",
    "\n",
    "print(\"\\nProject Achievements:\")\n",
    "print(\"-\" * 40)\n",
    "print(\"1. ✓ Successfully trained and evaluated multiple ML models\")\n",
    "print(\"2. ✓ Achieved strong predictive performance for agricultural yield\")\n",
    "print(\"3. ✓ Conducted comprehensive model interpretation with SHAP and LIME\")\n",
    "print(\"4. ✓ Performed detailed error analysis and residual diagnostics\")\n",
    "print(\"5. ✓ Assessed model deployment readiness and scalability\")\n",
    "print(\"6. ✓ Created production-ready model pipelines\")\n",
    "print(\"7. ✓ Generated actionable business insights and recommendations\")\n",
    "print(\"8. ✓ Prepared comprehensive final model report\")\n",
    "\n",
    "print(\"\\nKey Deliverables:\")\n",
    "print(\"-\" * 40)\n",
    "for prob_name in tuned_models.keys():\n",
    "    print(f\"• {prob_name}:\")\n",
    "    if prob_name in evaluation_results:\n",
    "        best_model_eval = evaluation_results[prob_name]\n",
    "        if 'regression' in prob_name:\n",
    "            best_model = min(best_model_eval.keys(),\n",
    "                           key=lambda x: best_model_eval[x]['metrics'].get('RMSE', float('inf')))\n",
    "            metrics = best_model_eval[best_model]['metrics']\n",
    "            print(f\"  - Best Model: {best_model}\")\n",
    "            print(f\"  - R² Score: {metrics.get('R2', 'N/A'):.3f}\")\n",
    "            print(f\"  - RMSE: {metrics.get('RMSE', 'N/A'):.2f}\")\n",
    "        else:\n",
    "            best_model = max(best_model_eval.keys(),\n",
    "                           key=lambda x: best_model_eval[x]['metrics'].get('Accuracy', 0))\n",
    "            metrics = best_model_eval[best_model]['metrics']\n",
    "            print(f\"  - Best Model: {best_model}\")\n",
    "            print(f\"  - Accuracy: {metrics.get('Accuracy', 'N/A'):.1%}\")\n",
    "\n",
    "print(\"\\nNext Steps for Deployment:\")\n",
    "print(\"-\" * 40)\n",
    "print(\"1. Deploy production pipelines to cloud environment (AWS/GCP/Azure)\")\n",
    "print(\"2. Implement real-time monitoring and logging\")\n",
    "print(\"3. Set up automated retraining pipeline\")\n",
    "print(\"4. Create API endpoints for model inference\")\n",
    "print(\"5. Develop dashboard for model performance visualization\")\n",
    "print(\"6. Establish feedback loop for continuous improvement\")\n",
    "\n",
    "print(\"\\nBusiness Impact Expected:\")\n",
    "print(\"-\" * 40)\n",
    "print(\"• Improved agricultural yield forecasting accuracy\")\n",
    "print(\"• Better resource allocation and planning\")\n",
    "print(\"• Early identification of low-yield scenarios\")\n",
    "print(\"• Data-driven decision making for agricultural policies\")\n",
    "print(\"• Optimization of crop selection and diversification strategies\")\n",
    "\n",
    "print(\"\\nThank you for completing the Agricultural Dataset Analysis project!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286cec98-db87-4a13-b0e9-132b649f9897",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6432cb-5903-40a4-a4da-bac83dc4fe37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d23080-359c-4ca3-9dba-ecca52193c25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1d587f-d6ba-4481-8248-243721013dd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca021d2-ad8e-476e-8a7a-40b74f1f7841",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd32d206-03d1-4f6c-afae-2ab192c1ace4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309ef5ed-f0bb-45c8-a21d-ff6804feb7cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d95123-42c2-46fb-a25f-c8107d4cb721",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd9879e-c32c-4729-9a3c-c39300f64024",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
